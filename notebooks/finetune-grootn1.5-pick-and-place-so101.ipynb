{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11132dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ðŸ°âœ¨ Everything looks OK!\n",
      "Channels:\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local\n",
      "\n",
      "  added / updated specs:\n",
      "    - ffmpeg=7.1.1\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    alsa-lib-1.2.15.3          |       hb03c661_0         571 KB  conda-forge\n",
      "    aom-3.9.1                  |       hac33072_0         2.6 MB  conda-forge\n",
      "    attr-2.5.2                 |       h39aace5_0          66 KB  conda-forge\n",
      "    ca-certificates-2026.1.4   |       hbd8a1cb_0         143 KB  conda-forge\n",
      "    cairo-1.18.4               |       he90730b_1         966 KB  conda-forge\n",
      "    certifi-2026.1.4           |     pyhd8ed1ab_0         147 KB  conda-forge\n",
      "    conda-24.11.3              |  py311h38be061_0         1.1 MB  conda-forge\n",
      "    dav1d-1.2.1                |       hd590300_0         742 KB  conda-forge\n",
      "    dbus-1.16.2                |       h24cb091_1         437 KB  conda-forge\n",
      "    ffmpeg-7.1.1               | gpl_hbbdf940_911        10.0 MB  conda-forge\n",
      "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
      "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
      "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
      "    font-ttf-ubuntu-0.83       |       h77eed37_3         1.5 MB  conda-forge\n",
      "    fontconfig-2.15.0          |       h7e30c49_1         259 KB  conda-forge\n",
      "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
      "    fonts-conda-forge-1        |       hc364b38_1           4 KB  conda-forge\n",
      "    freetype-2.14.1            |       ha770c72_0         169 KB  conda-forge\n",
      "    fribidi-1.0.16             |       hb03c661_0          60 KB  conda-forge\n",
      "    gdk-pixbuf-2.44.5          |       h2b0a6b4_0         562 KB  conda-forge\n",
      "    gmp-6.3.0                  |       hac33072_2         449 KB  conda-forge\n",
      "    graphite2-1.3.14           |       hecca717_2          97 KB  conda-forge\n",
      "    harfbuzz-12.3.2            |       h6083320_0         1.9 MB  conda-forge\n",
      "    icu-78.2                   |       h33c6efd_0        12.1 MB  conda-forge\n",
      "    intel-gmmlib-22.9.0        |       hb700be7_0         986 KB  conda-forge\n",
      "    intel-media-driver-25.3.4  |       hecca717_0         8.0 MB  conda-forge\n",
      "    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n",
      "    lerc-4.0.0                 |       h0aef613_1         258 KB  conda-forge\n",
      "    level-zero-1.28.0          |       hb700be7_0         657 KB  conda-forge\n",
      "    libabseil-20250512.1       | cxx17_hba17884_0         1.2 MB  conda-forge\n",
      "    libass-0.17.4              |       h96ad9f0_0         149 KB  conda-forge\n",
      "    libcap-2.77                |       h3ff7636_0         119 KB  conda-forge\n",
      "    libdeflate-1.25            |       h17f619e_0          72 KB  conda-forge\n",
      "    libdrm-2.4.125             |       hb03c661_1         304 KB  conda-forge\n",
      "    libegl-1.7.0               |       ha4b6fd6_2          44 KB  conda-forge\n",
      "    libexpat-2.7.3             |       hecca717_0          75 KB  conda-forge\n",
      "    libffi-3.5.2               |       h3435931_0          57 KB  conda-forge\n",
      "    libflac-1.5.0              |       he200343_1         415 KB  conda-forge\n",
      "    libfreetype-2.14.1         |       ha770c72_0           7 KB  conda-forge\n",
      "    libfreetype6-2.14.1        |       h73754d4_0         378 KB  conda-forge\n",
      "    libgl-1.7.0                |       ha4b6fd6_2         132 KB  conda-forge\n",
      "    libglib-2.86.3             |       h6548e54_0         3.8 MB  conda-forge\n",
      "    libglvnd-1.7.0             |       ha4b6fd6_2         129 KB  conda-forge\n",
      "    libglx-1.7.0               |       ha4b6fd6_2          74 KB  conda-forge\n",
      "    libhwloc-2.12.2            |default_hafda6a7_1000         2.3 MB  conda-forge\n",
      "    libiconv-1.18              |       h3b78370_2         772 KB  conda-forge\n",
      "    libjpeg-turbo-3.1.2        |       hb03c661_0         619 KB  conda-forge\n",
      "    liblzma-5.8.2              |       hb03c661_0         111 KB  conda-forge\n",
      "    libogg-1.3.5               |       hd0c01bc_1         213 KB  conda-forge\n",
      "    libopenvino-2025.2.0       |       hb617929_1         6.0 MB  conda-forge\n",
      "    libopenvino-auto-batch-plugin-2025.2.0|       hed573e4_1         112 KB  conda-forge\n",
      "    libopenvino-auto-plugin-2025.2.0|       hed573e4_1         245 KB  conda-forge\n",
      "    libopenvino-hetero-plugin-2025.2.0|       hd41364c_1         190 KB  conda-forge\n",
      "    libopenvino-intel-cpu-plugin-2025.2.0|       hb617929_1        11.8 MB  conda-forge\n",
      "    libopenvino-intel-gpu-plugin-2025.2.0|       hb617929_1        10.3 MB  conda-forge\n",
      "    libopenvino-intel-npu-plugin-2025.2.0|       hb617929_1         1.2 MB  conda-forge\n",
      "    libopenvino-ir-frontend-2025.2.0|       hd41364c_1         200 KB  conda-forge\n",
      "    libopenvino-onnx-frontend-2025.2.0|       h1862bb8_1         1.6 MB  conda-forge\n",
      "    libopenvino-paddle-frontend-2025.2.0|       h1862bb8_1         727 KB  conda-forge\n",
      "    libopenvino-pytorch-frontend-2025.2.0|       hecca717_1         1.2 MB  conda-forge\n",
      "    libopenvino-tensorflow-frontend-2025.2.0|       h0767aad_1         1.3 MB  conda-forge\n",
      "    libopenvino-tensorflow-lite-frontend-2025.2.0|       hecca717_1         485 KB  conda-forge\n",
      "    libopus-1.6.1              |       h280c20c_0         317 KB  conda-forge\n",
      "    libpciaccess-0.18          |       hb9d3cd8_0          28 KB  conda-forge\n",
      "    libpng-1.6.55              |       h421ea60_0         310 KB  conda-forge\n",
      "    libprotobuf-6.31.1         |       h49aed37_4         4.2 MB  conda-forge\n",
      "    librsvg-2.60.0             |       h61e6d4b_0         3.3 MB  conda-forge\n",
      "    libsndfile-1.2.2           |       hc7d488a_2         347 KB  conda-forge\n",
      "    libsystemd0-259.1          |       h6569c3e_0         511 KB  conda-forge\n",
      "    libtiff-4.7.1              |       h9d88235_1         425 KB  conda-forge\n",
      "    libudev1-259.1             |       h6569c3e_0         164 KB  conda-forge\n",
      "    libunwind-1.8.3            |       h65a8314_0          74 KB  conda-forge\n",
      "    liburing-2.13              |       hb700be7_0         129 KB  conda-forge\n",
      "    libusb-1.0.29              |       h73b1eb8_0          87 KB  conda-forge\n",
      "    libva-2.23.0               |       he1eb515_0         216 KB  conda-forge\n",
      "    libvorbis-1.3.7            |       h54a6638_2         279 KB  conda-forge\n",
      "    libvpl-2.15.0              |       h54a6638_1         281 KB  conda-forge\n",
      "    libvpx-1.14.1              |       hac33072_0         999 KB  conda-forge\n",
      "    libvulkan-loader-1.4.341.0 |       h5279c79_0         195 KB  conda-forge\n",
      "    libwebp-base-1.6.0         |       hd42ef1d_0         419 KB  conda-forge\n",
      "    libxcb-1.17.0              |       h8a09558_0         387 KB  conda-forge\n",
      "    libxkbcommon-1.13.1        |       hca5e8e5_0         818 KB  conda-forge\n",
      "    libxml2-2.15.1             |       he237659_1          44 KB  conda-forge\n",
      "    libxml2-16-2.15.1          |       hca6bf5a_1         543 KB  conda-forge\n",
      "    mpg123-1.32.9              |       hc50e24c_0         480 KB  conda-forge\n",
      "    ocl-icd-2.3.3              |       hb9d3cd8_0         104 KB  conda-forge\n",
      "    opencl-headers-2025.06.13  |       h5888daf_0          54 KB  conda-forge\n",
      "    openh264-2.6.0             |       hc22cd8d_0         714 KB  conda-forge\n",
      "    openssl-3.6.1              |       h35e630c_1         3.0 MB  conda-forge\n",
      "    pango-1.56.4               |       hadf4263_0         445 KB  conda-forge\n",
      "    pcre2-10.47                |       haa7fec5_0         1.2 MB  conda-forge\n",
      "    pixman-0.46.4              |       h54a6638_1         440 KB  conda-forge\n",
      "    pthread-stubs-0.4          |    hb9d3cd8_1002           8 KB  conda-forge\n",
      "    pugixml-1.15               |       h3f63f65_0         116 KB  conda-forge\n",
      "    pulseaudio-client-17.0     |       h9a6aba3_3         733 KB  conda-forge\n",
      "    sdl2-2.32.56               |       h54a6638_0         575 KB  conda-forge\n",
      "    sdl3-3.4.0                 |       h3b84278_0         2.0 MB  conda-forge\n",
      "    snappy-1.2.2               |       h03e3b7b_1          45 KB  conda-forge\n",
      "    svt-av1-3.1.2              |       hecca717_0         2.6 MB  conda-forge\n",
      "    tbb-2022.3.0               |       hb700be7_2         177 KB  conda-forge\n",
      "    wayland-1.24.0             |       hd6090a7_1         322 KB  conda-forge\n",
      "    wayland-protocols-1.47     |       hd8ed1ab_0         137 KB  conda-forge\n",
      "    x264-1!164.3095            |       h166bdaf_2         877 KB  conda-forge\n",
      "    x265-3.5                   |       h924138e_3         3.2 MB  conda-forge\n",
      "    xkeyboard-config-2.46      |       hb03c661_0         388 KB  conda-forge\n",
      "    xorg-libice-1.1.2          |       hb9d3cd8_0          57 KB  conda-forge\n",
      "    xorg-libsm-1.2.6           |       he73a12e_0          27 KB  conda-forge\n",
      "    xorg-libx11-1.8.13         |       he1eb515_0         820 KB  conda-forge\n",
      "    xorg-libxau-1.0.12         |       hb03c661_1          15 KB  conda-forge\n",
      "    xorg-libxcursor-1.2.3      |       hb9d3cd8_0          32 KB  conda-forge\n",
      "    xorg-libxdmcp-1.1.5        |       hb03c661_1          20 KB  conda-forge\n",
      "    xorg-libxext-1.3.7         |       hb03c661_0          49 KB  conda-forge\n",
      "    xorg-libxfixes-6.0.2       |       hb03c661_0          20 KB  conda-forge\n",
      "    xorg-libxi-1.8.2           |       hb9d3cd8_0          46 KB  conda-forge\n",
      "    xorg-libxrandr-1.5.5       |       hb03c661_0          30 KB  conda-forge\n",
      "    xorg-libxrender-0.9.12     |       hb9d3cd8_0          32 KB  conda-forge\n",
      "    xorg-libxscrnsaver-1.2.4   |       hb9d3cd8_0          14 KB  conda-forge\n",
      "    xorg-libxtst-1.2.5         |       hb9d3cd8_3          32 KB  conda-forge\n",
      "    zstandard-0.25.0           |  py311haee01d2_1         456 KB  conda-forge\n",
      "    zstd-1.5.7                 |       hb78ec9c_6         587 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       125.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  alsa-lib           conda-forge/linux-64::alsa-lib-1.2.15.3-hb03c661_0 \n",
      "  aom                conda-forge/linux-64::aom-3.9.1-hac33072_0 \n",
      "  attr               conda-forge/linux-64::attr-2.5.2-h39aace5_0 \n",
      "  cairo              conda-forge/linux-64::cairo-1.18.4-he90730b_1 \n",
      "  dav1d              conda-forge/linux-64::dav1d-1.2.1-hd590300_0 \n",
      "  dbus               conda-forge/linux-64::dbus-1.16.2-h24cb091_1 \n",
      "  ffmpeg             conda-forge/linux-64::ffmpeg-7.1.1-gpl_hbbdf940_911 \n",
      "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
      "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
      "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
      "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_3 \n",
      "  fontconfig         conda-forge/linux-64::fontconfig-2.15.0-h7e30c49_1 \n",
      "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
      "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-hc364b38_1 \n",
      "  freetype           conda-forge/linux-64::freetype-2.14.1-ha770c72_0 \n",
      "  fribidi            conda-forge/linux-64::fribidi-1.0.16-hb03c661_0 \n",
      "  gdk-pixbuf         conda-forge/linux-64::gdk-pixbuf-2.44.5-h2b0a6b4_0 \n",
      "  gmp                conda-forge/linux-64::gmp-6.3.0-hac33072_2 \n",
      "  graphite2          conda-forge/linux-64::graphite2-1.3.14-hecca717_2 \n",
      "  harfbuzz           conda-forge/linux-64::harfbuzz-12.3.2-h6083320_0 \n",
      "  icu                conda-forge/linux-64::icu-78.2-h33c6efd_0 \n",
      "  intel-gmmlib       conda-forge/linux-64::intel-gmmlib-22.9.0-hb700be7_0 \n",
      "  intel-media-driver conda-forge/linux-64::intel-media-driver-25.3.4-hecca717_0 \n",
      "  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 \n",
      "  lerc               conda-forge/linux-64::lerc-4.0.0-h0aef613_1 \n",
      "  level-zero         conda-forge/linux-64::level-zero-1.28.0-hb700be7_0 \n",
      "  libabseil          conda-forge/linux-64::libabseil-20250512.1-cxx17_hba17884_0 \n",
      "  libass             conda-forge/linux-64::libass-0.17.4-h96ad9f0_0 \n",
      "  libcap             conda-forge/linux-64::libcap-2.77-h3ff7636_0 \n",
      "  libdeflate         conda-forge/linux-64::libdeflate-1.25-h17f619e_0 \n",
      "  libdrm             conda-forge/linux-64::libdrm-2.4.125-hb03c661_1 \n",
      "  libegl             conda-forge/linux-64::libegl-1.7.0-ha4b6fd6_2 \n",
      "  libflac            conda-forge/linux-64::libflac-1.5.0-he200343_1 \n",
      "  libfreetype        conda-forge/linux-64::libfreetype-2.14.1-ha770c72_0 \n",
      "  libfreetype6       conda-forge/linux-64::libfreetype6-2.14.1-h73754d4_0 \n",
      "  libgl              conda-forge/linux-64::libgl-1.7.0-ha4b6fd6_2 \n",
      "  libglib            conda-forge/linux-64::libglib-2.86.3-h6548e54_0 \n",
      "  libglvnd           conda-forge/linux-64::libglvnd-1.7.0-ha4b6fd6_2 \n",
      "  libglx             conda-forge/linux-64::libglx-1.7.0-ha4b6fd6_2 \n",
      "  libhwloc           conda-forge/linux-64::libhwloc-2.12.2-default_hafda6a7_1000 \n",
      "  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-3.1.2-hb03c661_0 \n",
      "  libogg             conda-forge/linux-64::libogg-1.3.5-hd0c01bc_1 \n",
      "  libopenvino        conda-forge/linux-64::libopenvino-2025.2.0-hb617929_1 \n",
      "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-batch-plugin-2025.2.0-hed573e4_1 \n",
      "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-plugin-2025.2.0-hed573e4_1 \n",
      "  libopenvino-heter~ conda-forge/linux-64::libopenvino-hetero-plugin-2025.2.0-hd41364c_1 \n",
      "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-cpu-plugin-2025.2.0-hb617929_1 \n",
      "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-gpu-plugin-2025.2.0-hb617929_1 \n",
      "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-npu-plugin-2025.2.0-hb617929_1 \n",
      "  libopenvino-ir-fr~ conda-forge/linux-64::libopenvino-ir-frontend-2025.2.0-hd41364c_1 \n",
      "  libopenvino-onnx-~ conda-forge/linux-64::libopenvino-onnx-frontend-2025.2.0-h1862bb8_1 \n",
      "  libopenvino-paddl~ conda-forge/linux-64::libopenvino-paddle-frontend-2025.2.0-h1862bb8_1 \n",
      "  libopenvino-pytor~ conda-forge/linux-64::libopenvino-pytorch-frontend-2025.2.0-hecca717_1 \n",
      "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-frontend-2025.2.0-h0767aad_1 \n",
      "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-lite-frontend-2025.2.0-hecca717_1 \n",
      "  libopus            conda-forge/linux-64::libopus-1.6.1-h280c20c_0 \n",
      "  libpciaccess       conda-forge/linux-64::libpciaccess-0.18-hb9d3cd8_0 \n",
      "  libpng             conda-forge/linux-64::libpng-1.6.55-h421ea60_0 \n",
      "  libprotobuf        conda-forge/linux-64::libprotobuf-6.31.1-h49aed37_4 \n",
      "  librsvg            conda-forge/linux-64::librsvg-2.60.0-h61e6d4b_0 \n",
      "  libsndfile         conda-forge/linux-64::libsndfile-1.2.2-hc7d488a_2 \n",
      "  libsystemd0        conda-forge/linux-64::libsystemd0-259.1-h6569c3e_0 \n",
      "  libtiff            conda-forge/linux-64::libtiff-4.7.1-h9d88235_1 \n",
      "  libudev1           conda-forge/linux-64::libudev1-259.1-h6569c3e_0 \n",
      "  libunwind          conda-forge/linux-64::libunwind-1.8.3-h65a8314_0 \n",
      "  liburing           conda-forge/linux-64::liburing-2.13-hb700be7_0 \n",
      "  libusb             conda-forge/linux-64::libusb-1.0.29-h73b1eb8_0 \n",
      "  libva              conda-forge/linux-64::libva-2.23.0-he1eb515_0 \n",
      "  libvorbis          conda-forge/linux-64::libvorbis-1.3.7-h54a6638_2 \n",
      "  libvpl             conda-forge/linux-64::libvpl-2.15.0-h54a6638_1 \n",
      "  libvpx             conda-forge/linux-64::libvpx-1.14.1-hac33072_0 \n",
      "  libvulkan-loader   conda-forge/linux-64::libvulkan-loader-1.4.341.0-h5279c79_0 \n",
      "  libwebp-base       conda-forge/linux-64::libwebp-base-1.6.0-hd42ef1d_0 \n",
      "  libxcb             conda-forge/linux-64::libxcb-1.17.0-h8a09558_0 \n",
      "  libxkbcommon       conda-forge/linux-64::libxkbcommon-1.13.1-hca5e8e5_0 \n",
      "  libxml2-16         conda-forge/linux-64::libxml2-16-2.15.1-hca6bf5a_1 \n",
      "  mpg123             conda-forge/linux-64::mpg123-1.32.9-hc50e24c_0 \n",
      "  ocl-icd            conda-forge/linux-64::ocl-icd-2.3.3-hb9d3cd8_0 \n",
      "  opencl-headers     conda-forge/linux-64::opencl-headers-2025.06.13-h5888daf_0 \n",
      "  openh264           conda-forge/linux-64::openh264-2.6.0-hc22cd8d_0 \n",
      "  pango              conda-forge/linux-64::pango-1.56.4-hadf4263_0 \n",
      "  pcre2              conda-forge/linux-64::pcre2-10.47-haa7fec5_0 \n",
      "  pixman             conda-forge/linux-64::pixman-0.46.4-h54a6638_1 \n",
      "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-hb9d3cd8_1002 \n",
      "  pugixml            conda-forge/linux-64::pugixml-1.15-h3f63f65_0 \n",
      "  pulseaudio-client  conda-forge/linux-64::pulseaudio-client-17.0-h9a6aba3_3 \n",
      "  sdl2               conda-forge/linux-64::sdl2-2.32.56-h54a6638_0 \n",
      "  sdl3               conda-forge/linux-64::sdl3-3.4.0-h3b84278_0 \n",
      "  snappy             conda-forge/linux-64::snappy-1.2.2-h03e3b7b_1 \n",
      "  svt-av1            conda-forge/linux-64::svt-av1-3.1.2-hecca717_0 \n",
      "  tbb                conda-forge/linux-64::tbb-2022.3.0-hb700be7_2 \n",
      "  wayland            conda-forge/linux-64::wayland-1.24.0-hd6090a7_1 \n",
      "  wayland-protocols  conda-forge/noarch::wayland-protocols-1.47-hd8ed1ab_0 \n",
      "  x264               conda-forge/linux-64::x264-1!164.3095-h166bdaf_2 \n",
      "  x265               conda-forge/linux-64::x265-3.5-h924138e_3 \n",
      "  xkeyboard-config   conda-forge/linux-64::xkeyboard-config-2.46-hb03c661_0 \n",
      "  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.2-hb9d3cd8_0 \n",
      "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.6-he73a12e_0 \n",
      "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.13-he1eb515_0 \n",
      "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.12-hb03c661_1 \n",
      "  xorg-libxcursor    conda-forge/linux-64::xorg-libxcursor-1.2.3-hb9d3cd8_0 \n",
      "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.5-hb03c661_1 \n",
      "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.7-hb03c661_0 \n",
      "  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-6.0.2-hb03c661_0 \n",
      "  xorg-libxi         conda-forge/linux-64::xorg-libxi-1.8.2-hb9d3cd8_0 \n",
      "  xorg-libxrandr     conda-forge/linux-64::xorg-libxrandr-1.5.5-hb03c661_0 \n",
      "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.12-hb9d3cd8_0 \n",
      "  xorg-libxscrnsaver conda-forge/linux-64::xorg-libxscrnsaver-1.2.4-hb9d3cd8_0 \n",
      "  xorg-libxtst       conda-forge/linux-64::xorg-libxtst-1.2.5-hb9d3cd8_3 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge/linux-64::ca-certificates~ --> conda-forge/noarch::ca-certificates-2026.1.4-hbd8a1cb_0 \n",
      "  certifi                           2024.12.14-pyhd8ed1ab_0 --> 2026.1.4-pyhd8ed1ab_0 \n",
      "  conda                             24.11.2-py311h38be061_1 --> 24.11.3-py311h38be061_0 \n",
      "  libexpat                                 2.6.4-h5888daf_0 --> 2.7.3-hecca717_0 \n",
      "  libffi                                   3.4.2-h7f98852_5 --> 3.5.2-h3435931_0 \n",
      "  libiconv                                  1.17-hd590300_2 --> 1.18-h3b78370_2 \n",
      "  liblzma                                  5.6.3-hb9d3cd8_1 --> 5.8.2-hb03c661_0 \n",
      "  libxml2                                 2.13.5-h0d44e9d_1 --> 2.15.1-he237659_1 \n",
      "  openssl                                  3.4.0-h7b32b05_1 --> 3.6.1-h35e630c_1 \n",
      "  zstandard                          0.23.0-py311hbc35293_1 --> 0.25.0-py311haee01d2_1 \n",
      "  zstd                                     1.5.6-ha6fb4c9_0 --> 1.5.7-hb78ec9c_6 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "icu-78.2             | 12.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
      "libopenvino-intel-cp | 11.8 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "libopenvino-intel-gp | 10.3 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ffmpeg-7.1.1         | 10.0 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "intel-media-driver-2 | 8.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-2025.2.0 | 6.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libprotobuf-6.31.1   | 4.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.3       | 3.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "librsvg-2.60.0       | 3.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x265-3.5             | 3.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.6.1        | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "svt-av1-3.1.2        | 2.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aom-3.9.1            | 2.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libhwloc-2.12.2      | 2.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sdl3-3.4.0           | 2.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "harfbuzz-12.3.2      | 1.9 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-onnx-fro | 1.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-tensorfl | 1.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-78.2             | 12.1 MB   | :   0% 0.001287195725793685/1 [00:00<01:25, 85.70s/it]\n",
      "\n",
      "\n",
      "\n",
      "intel-media-driver-2 | 8.0 MB    | :   0% 0.0019447784526524076/1 [00:00<00:52, 53.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libopenvino-intel-gp | 10.3 MB   | :   0% 0.0015148657294914325/1 [00:00<01:13, 73.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ffmpeg-7.1.1         | 10.0 MB   | :   0% 0.001555348585002069/1 [00:00<01:18, 78.67s/it]\u001b[A\u001b[A\u001b[A\n",
      "icu-78.2             | 12.1 MB   | :  18% 0.17505861870794115/1 [00:00<00:00,  1.02s/it] \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "intel-media-driver-2 | 8.0 MB    | :  30% 0.29755110325581835/1 [00:00<00:00,  1.72it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libopenvino-intel-gp | 10.3 MB   | :  22% 0.22420012796473204/1 [00:00<00:00,  1.26it/s]  \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ffmpeg-7.1.1         | 10.0 MB   | :  26% 0.2612985622803476/1 [00:00<00:00,  1.40it/s]  \u001b[A\u001b[A\u001b[A\n",
      "icu-78.2             | 12.1 MB   | :  43% 0.4286361766892971/1 [00:00<00:00,  1.67it/s]  \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "intel-media-driver-2 | 8.0 MB    | :  70% 0.7001202429548667/1 [00:00<00:00,  2.77it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libopenvino-intel-gp | 10.3 MB   | :  53% 0.5271732738630186/1 [00:00<00:00,  2.04it/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ffmpeg-7.1.1         | 10.0 MB   | :  58% 0.5785896736207696/1 [00:00<00:00,  2.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "icu-78.2             | 12.1 MB   | :  73% 0.7349887594281941/1 [00:00<00:00,  2.21it/s]]\u001b[A\n",
      "\n",
      "libopenvino-intel-gp | 10.3 MB   | :  80% 0.8043937023599508/1 [00:00<00:00,  2.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ffmpeg-7.1.1         | 10.0 MB   | :  91% 0.9083235736412083/1 [00:00<00:00,  2.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "libopenvino-intel-cp | 11.8 MB   | :  78% 0.7836265322979913/1 [00:00<00:00,  2.15it/s] \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "intel-media-driver-2 | 8.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.92it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "intel-media-driver-2 | 8.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-2025.2.0 | 6.0 MB    | :   0% 0.0026236350380182075/1 [00:00<03:37, 218.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libopenvino-intel-gp | 10.3 MB   | : 100% 1.0/1 [00:00<00:00,  2.32it/s]               \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ffmpeg-7.1.1         | 10.0 MB   | : 100% 1.0/1 [00:00<00:00,  2.60it/s]               \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-2025.2.0 | 6.0 MB    | :  46% 0.45913613165318634/1 [00:00<00:00,  1.10s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icu-78.2             | 12.1 MB   | : 100% 1.0/1 [00:00<00:00,  1.23it/s]               /it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.3       | 3.8 MB    | :   0% 0.00415148248770696/1 [00:00<03:02, 182.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-2025.2.0 | 6.0 MB    | :  96% 0.9628740589526822/1 [00:00<00:00,  1.80it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libprotobuf-6.31.1   | 4.2 MB    | :  40% 0.4009277821916499/1 [00:00<00:00,  1.48s/it]    \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "librsvg-2.60.0       | 3.3 MB    | :   0% 0.004787875546796486/1 [00:00<02:47, 168.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.3       | 3.8 MB    | :  71% 0.7140549878855971/1 [00:00<00:00,  1.13it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "libopenvino-intel-cp | 11.8 MB   | : 100% 1.0/1 [00:00<00:00,  2.15it/s]               \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libprotobuf-6.31.1   | 4.2 MB    | :  97% 0.966723063602296/1 [00:00<00:00,  1.65it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "librsvg-2.60.0       | 3.3 MB    | :  65% 0.6511510743643222/1 [00:00<00:00,  1.02s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-2025.2.0 | 6.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.80it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x265-3.5             | 3.2 MB    | :   0% 0.004880274801411181/1 [00:00<03:09, 190.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.3       | 3.8 MB    | : 100% 1.0/1 [00:00<00:00,  1.13it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libprotobuf-6.31.1   | 4.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.65it/s]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.6.1        | 3.0 MB    | :   1% 0.005177353754134473/1 [00:00<03:10, 191.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "svt-av1-3.1.2        | 2.6 MB    | :   1% 0.005976944403910696/1 [00:01<02:53, 174.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "librsvg-2.60.0       | 3.3 MB    | : 100% 1.0/1 [00:01<00:00,  1.02s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aom-3.9.1            | 2.6 MB    | :   1% 0.006053807351178468/1 [00:01<02:53, 174.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.6.1        | 3.0 MB    | :  79% 0.7869577706284399/1 [00:01<00:00,  1.01s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x265-3.5             | 3.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.20it/s]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x265-3.5             | 3.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libhwloc-2.12.2      | 2.3 MB    | :   1% 0.006687576227103297/1 [00:01<02:45, 166.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aom-3.9.1            | 2.6 MB    | :  96% 0.9625553688373764/1 [00:01<00:00,  1.14it/s]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sdl3-3.4.0           | 2.0 MB    | :   1% 0.007671475073324768/1 [00:01<02:30, 151.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "svt-av1-3.1.2        | 2.6 MB    | : 100% 1.0/1 [00:01<00:00,  1.12it/s]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "svt-av1-3.1.2        | 2.6 MB    | : 100% 1.0/1 [00:01<00:00,  1.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.6.1        | 3.0 MB    | : 100% 1.0/1 [00:01<00:00,  1.01s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aom-3.9.1            | 2.6 MB    | : 100% 1.0/1 [00:01<00:00,  1.14it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "harfbuzz-12.3.2      | 1.9 MB    | :   1% 0.008047708608503831/1 [00:01<02:35, 156.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sdl3-3.4.0           | 2.0 MB    | :  96% 0.958934384165596/1 [00:01<00:00,  1.05it/s]    \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libhwloc-2.12.2      | 2.3 MB    | : 100% 1.0/1 [00:01<00:00,  1.07it/s]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libhwloc-2.12.2      | 2.3 MB    | : 100% 1.0/1 [00:01<00:00,  1.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | :   1% 0.010110434778315882/1 [00:01<02:05, 127.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-onnx-fro | 1.6 MB    | :   1% 0.00950070832002032/1 [00:01<02:14, 135.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sdl3-3.4.0           | 2.0 MB    | : 100% 1.0/1 [00:01<00:00,  1.05it/s]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-tensorfl | 1.3 MB    | :   1% 0.012364732438329161/1 [00:01<01:46, 107.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "harfbuzz-12.3.2      | 1.9 MB    | : 100% 1.0/1 [00:01<00:00,  1.00it/s]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "harfbuzz-12.3.2      | 1.9 MB    | : 100% 1.0/1 [00:01<00:00,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-onnx-fro | 1.6 MB    | : 100% 1.0/1 [00:01<00:00, 135.54s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | : 100% 1.0/1 [00:01<00:00,  1.02s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | : 100% 1.0/1 [00:01<00:00,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-tensorfl | 1.3 MB    | : 100% 1.0/1 [00:01<00:00, 107.36s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "intel-media-driver-2 | 8.0 MB    | : 100% 1.0/1 [00:01<00:00,  1.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libopenvino-intel-gp | 10.3 MB   | : 100% 1.0/1 [00:02<00:00,  2.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "icu-78.2             | 12.1 MB   | : 100% 1.0/1 [00:03<00:00,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "libopenvino-intel-cp | 11.8 MB   | : 100% 1.0/1 [00:03<00:00,  2.15it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-2025.2.0 | 6.0 MB    | : 100% 1.0/1 [00:03<00:00,  1.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libglib-2.86.3       | 3.8 MB    | : 100% 1.0/1 [00:03<00:00,  1.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libprotobuf-6.31.1   | 4.2 MB    | : 100% 1.0/1 [00:03<00:00,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "librsvg-2.60.0       | 3.3 MB    | : 100% 1.0/1 [00:03<00:00,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "svt-av1-3.1.2        | 2.6 MB    | : 100% 1.0/1 [00:03<00:00,  1.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.6.1        | 3.0 MB    | : 100% 1.0/1 [00:04<00:00,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "aom-3.9.1            | 2.6 MB    | : 100% 1.0/1 [00:04<00:00,  1.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libhwloc-2.12.2      | 2.3 MB    | : 100% 1.0/1 [00:04<00:00,  1.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sdl3-3.4.0           | 2.0 MB    | : 100% 1.0/1 [00:04<00:00,  1.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "harfbuzz-12.3.2      | 1.9 MB    | : 100% 1.0/1 [00:04<00:00,  1.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-onnx-fro | 1.6 MB    | : 100% 1.0/1 [00:04<00:00,  4.28s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-onnx-fro | 1.6 MB    | : 100% 1.0/1 [00:04<00:00,  4.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "font-ttf-ubuntu-0.83 | 1.5 MB    | : 100% 1.0/1 [00:04<00:00,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-tensorfl | 1.3 MB    | : 100% 1.0/1 [00:04<00:00,  4.38s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenvino-tensorfl | 1.3 MB    | : 100% 1.0/1 [00:04<00:00,  4.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x265-3.5             | 3.2 MB    | : 100% 1.0/1 [00:05<00:00,  1.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \n",
      "                                                                        \u001b[A\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: - \n",
      "\\ \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%pip install condacolab --quiet\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "!conda install ffmpeg=7.1.1 -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb3df90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.6/39.6 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m952.1/952.1 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.5/47.5 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for evdev (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install lerobot --quiet\n",
    "%pip install lerobot[groot] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8bcafe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lerobot@ git+https://github.com/huggingface/lerobot.git (from lerobot[pi]@ git+https://github.com/huggingface/lerobot.git)\n",
      "  Cloning https://github.com/huggingface/lerobot.git to /tmp/pip-install-kz39guz2/lerobot_4e48e7c1e3b248c08196f9a84e100e30\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/lerobot.git /tmp/pip-install-kz39guz2/lerobot_4e48e7c1e3b248c08196f9a84e100e30\n",
      "  Resolved https://github.com/huggingface/lerobot.git to commit fc8a388a2538937992bd8b28bc7ac909ebd1b9a0\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: datasets<4.2.0,>=4.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (4.1.1)\n",
      "Requirement already satisfied: diffusers<0.36.0,>=0.27.2 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.35.2)\n",
      "Requirement already satisfied: huggingface-hub<0.36.0,>=0.34.2 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.35.3)\n",
      "Requirement already satisfied: accelerate<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.12.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=71.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (80.10.2)\n",
      "Requirement already satisfied: cmake<4.2.0,>=3.29.0.1 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (4.1.3)\n",
      "Requirement already satisfied: einops<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.8.2)\n",
      "Requirement already satisfied: opencv-python-headless<4.13.0,>=4.9.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (4.12.0.88)\n",
      "Requirement already satisfied: av<16.0.0,>=15.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (15.1.0)\n",
      "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (4.0.0)\n",
      "Requirement already satisfied: packaging<26.0,>=24.2 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (24.2)\n",
      "Requirement already satisfied: pynput<1.9.0,>=1.7.7 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.8.1)\n",
      "Requirement already satisfied: pyserial<4.0,>=3.5 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.5)\n",
      "Requirement already satisfied: wandb<0.25.0,>=0.24.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.24.2)\n",
      "Requirement already satisfied: torch<2.8.0,>=2.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.7.1)\n",
      "Requirement already satisfied: torchcodec<0.6.0,>=0.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.5)\n",
      "Requirement already satisfied: torchvision<0.23.0,>=0.21.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.22.1)\n",
      "Requirement already satisfied: draccus==0.10.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.10.0)\n",
      "Requirement already satisfied: gymnasium<2.0.0,>=1.1.1 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.2.3)\n",
      "Requirement already satisfied: rerun-sdk<0.27.0,>=0.24.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.26.2)\n",
      "Requirement already satisfied: deepdiff<9.0.0,>=7.0.1 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (8.6.1)\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.34.0 in /usr/local/lib/python3.11/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.37.2)\n",
      "Requirement already satisfied: termcolor<4.0.0,>=2.4.0 in /usr/local/lib/python3.11/site-packages (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.3.0)\n",
      "Requirement already satisfied: mergedeep~=1.3 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.3.4)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (6.0.3)\n",
      "Requirement already satisfied: pyyaml-include~=1.4 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.4.1)\n",
      "Requirement already satisfied: toml~=0.10 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.10.2)\n",
      "Requirement already satisfied: typing-inspect~=0.9.0 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.9.0)\n",
      "Collecting transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git)\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision fix/lerobot_openpi) to /tmp/pip-install-kz39guz2/transformers_27e712021f424707a5b6d94b32b5d771\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-kz39guz2/transformers_27e712021f424707a5b6d94b32b5d771\n",
      "  Running command git checkout -b fix/lerobot_openpi --track origin/fix/lerobot_openpi\n",
      "  Switched to a new branch 'fix/lerobot_openpi'\n",
      "  Branch 'fix/lerobot_openpi' set up to track remote branch 'fix/lerobot_openpi' from 'origin'.\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit dcddb970176382c0fcf4521b0c0e6fc15894dfe0\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting scipy<1.15,>=1.10.1 (from lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.2.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (7.2.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/site-packages (from datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2025.9.0)\n",
      "Requirement already satisfied: orderly-set<6,>=5.4.1 in /usr/local/lib/python3.11/site-packages (from deepdiff<9.0.0,>=7.0.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (5.5.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (8.7.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2026.1.15)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (12.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.0.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<0.36.0,>=0.34.2->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.2.0)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.3.4)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.1.9)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.0.52)\n",
      "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.6.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/site-packages (from jsonlines<5.0.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (25.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from pynput<1.9.0,>=1.7.7->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.17.0)\n",
      "Requirement already satisfied: evdev>=1.3 in /usr/local/lib/python3.11/site-packages (from pynput<1.9.0,>=1.7.7->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.9.3)\n",
      "Requirement already satisfied: python-xlib>=0.17 in /usr/local/lib/python3.11/site-packages (from pynput<1.9.0,>=1.7.7->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.33)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/site-packages (from torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.3.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/site-packages (from wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.1.46)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/site-packages (from wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/site-packages (from wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (6.33.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/site-packages (from wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.12.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/site-packages (from wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.52.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.13.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/site-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/site-packages (from importlib_metadata->diffusers<0.36.0,>=0.27.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch<2.8.0,>=2.2.1->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (1.22.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.25.0,>=0.24.0->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (5.0.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot@ git+https://github.com/huggingface/lerobot.git->lerobot[pi]@ git+https://github.com/huggingface/lerobot.git) (0.6.0)\n",
      "Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lerobot, transformers\n",
      "  Building wheel for lerobot (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lerobot: filename=lerobot-0.4.4-py3-none-any.whl size=1051999 sha256=9d9ed9125fd9fad2b6ae8a71bd05c97d22cdbceb0770d616593683b4c067e000\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-e1nfkfcz/wheels/ea/11/7f/4043743d6976d3ca643f20d8ba4b694b65a1034decc0d7568d\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.53.3-py3-none-any.whl size=10827916 sha256=34079db6ff6b055f0e702c437793e248e33d895b44bed7ac135404a35cd7d180\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-e1nfkfcz/wheels/9d/51/28/98eacb664aa2bebef7b2839f251d4c01d0758064ac05c683f5\n",
      "Successfully built lerobot transformers\n",
      "Installing collected packages: scipy, tokenizers, transformers, lerobot\n",
      "  Attempting uninstall: lerobot\n",
      "    Found existing installation: lerobot 0.4.3\n",
      "    Uninstalling lerobot-0.4.3:\n",
      "      Successfully uninstalled lerobot-0.4.3\n",
      "Successfully installed lerobot-0.4.4 scipy-1.14.1 tokenizers-0.21.4 transformers-4.53.3\n"
     ]
    }
   ],
   "source": [
    "%pip install \"lerobot[pi]@git+https://github.com/huggingface/lerobot.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56edb89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: lerobot-train [-h] [--config_path str] [--dataset str]\n",
      "                     [--dataset.repo_id str] [--dataset.root [str]]\n",
      "                     [--dataset.episodes [List]] [--image_transforms str]\n",
      "                     [--dataset.image_transforms.enable bool]\n",
      "                     [--dataset.image_transforms.max_num_transforms int]\n",
      "                     [--dataset.image_transforms.random_order bool]\n",
      "                     [--dataset.image_transforms.tfs Dict]\n",
      "                     [--dataset.revision [str]]\n",
      "                     [--dataset.use_imagenet_stats bool]\n",
      "                     [--dataset.video_backend str] [--dataset.streaming bool]\n",
      "                     [--env str]\n",
      "                     [--env.type {aloha,pusht,gym_manipulator,libero,metaworld,isaaclab_arena}]\n",
      "                     [--env.visualization_width int]\n",
      "                     [--env.visualization_height int] [--robot str]\n",
      "                     [--env.robot.type {}] [--teleop str]\n",
      "                     [--env.teleop.type {}] [--processor str]\n",
      "                     [--env.processor.control_mode str] [--observation str]\n",
      "                     [--env.processor.observation.add_joint_velocity_to_observation bool]\n",
      "                     [--env.processor.observation.add_current_to_observation bool]\n",
      "                     [--env.processor.observation.display_cameras bool]\n",
      "                     [--image_preprocessing str]\n",
      "                     [--env.processor.image_preprocessing.crop_params_dict [Dict]]\n",
      "                     [--env.processor.image_preprocessing.resize_size [int int]]\n",
      "                     [--gripper str]\n",
      "                     [--env.processor.gripper.use_gripper bool]\n",
      "                     [--env.processor.gripper.gripper_penalty float]\n",
      "                     [--reset str]\n",
      "                     [--env.processor.reset.fixed_reset_joint_positions [Any]]\n",
      "                     [--env.processor.reset.reset_time_s float]\n",
      "                     [--env.processor.reset.control_time_s float]\n",
      "                     [--env.processor.reset.terminate_on_success bool]\n",
      "                     [--inverse_kinematics str]\n",
      "                     [--env.processor.inverse_kinematics.urdf_path [str]]\n",
      "                     [--env.processor.inverse_kinematics.target_frame_name [str]]\n",
      "                     [--env.processor.inverse_kinematics.end_effector_bounds [Dict]]\n",
      "                     [--env.processor.inverse_kinematics.end_effector_step_sizes [Dict]]\n",
      "                     [--reward_classifier str]\n",
      "                     [--env.processor.reward_classifier.pretrained_path [str]]\n",
      "                     [--env.processor.reward_classifier.success_threshold float]\n",
      "                     [--env.processor.reward_classifier.success_reward float]\n",
      "                     [--env.processor.max_gripper_pos [float]]\n",
      "                     [--env.name str] [--env.camera_name str]\n",
      "                     [--env.init_states bool]\n",
      "                     [--env.camera_name_mapping [Dict]]\n",
      "                     [--env.observation_height int]\n",
      "                     [--env.observation_width int] [--env.control_mode str]\n",
      "                     [--env.obs_type str] [--env.render_mode str]\n",
      "                     [--env.multitask_eval bool] [--env.task [str]]\n",
      "                     [--env.fps int] [--env.features Dict]\n",
      "                     [--env.features_map Dict] [--env.max_parallel_tasks int]\n",
      "                     [--env.disable_env_checker bool] [--env.hub_path str]\n",
      "                     [--env.episode_length int] [--env.num_envs int]\n",
      "                     [--env.embodiment [str]] [--env.object [str]]\n",
      "                     [--env.mimic bool] [--env.teleop_device [str]]\n",
      "                     [--env.seed [int]] [--env.device [str]]\n",
      "                     [--env.disable_fabric bool] [--env.enable_cameras bool]\n",
      "                     [--env.headless bool] [--env.enable_pinocchio bool]\n",
      "                     [--env.environment [str]] [--env.state_dim int]\n",
      "                     [--env.action_dim int] [--env.camera_height int]\n",
      "                     [--env.camera_width int] [--env.video bool]\n",
      "                     [--env.video_length int] [--env.video_interval int]\n",
      "                     [--env.state_keys str] [--env.camera_keys [str]]\n",
      "                     [--env.kwargs [dict]] [--policy str]\n",
      "                     [--policy.type {act,diffusion,groot,pi0,pi0_fast,pi05,smolvla,tdmpc,vqbet,wall_x,xvla,sac,reward_classifier,sarm}]\n",
      "                     [--policy.replace_final_stride_with_dilation int]\n",
      "                     [--policy.pre_norm bool] [--policy.dim_model int]\n",
      "                     [--policy.n_heads int] [--policy.dim_feedforward int]\n",
      "                     [--policy.feedforward_activation str]\n",
      "                     [--policy.n_encoder_layers int]\n",
      "                     [--policy.n_decoder_layers int] [--policy.use_vae bool]\n",
      "                     [--policy.n_vae_encoder_layers int]\n",
      "                     [--policy.temporal_ensemble_coeff [float]]\n",
      "                     [--policy.kl_weight float]\n",
      "                     [--policy.optimizer_lr_backbone float]\n",
      "                     [--policy.use_separate_rgb_encoder_per_camera bool]\n",
      "                     [--policy.down_dims int [int, ...]]\n",
      "                     [--policy.kernel_size int] [--policy.n_groups int]\n",
      "                     [--policy.diffusion_step_embed_dim int]\n",
      "                     [--policy.use_film_scale_modulation bool]\n",
      "                     [--policy.noise_scheduler_type str]\n",
      "                     [--policy.num_train_timesteps int]\n",
      "                     [--policy.beta_schedule str] [--policy.beta_start float]\n",
      "                     [--policy.beta_end float] [--policy.prediction_type str]\n",
      "                     [--policy.clip_sample bool]\n",
      "                     [--policy.clip_sample_range float]\n",
      "                     [--policy.do_mask_loss_for_padding bool]\n",
      "                     [--policy.scheduler_name str]\n",
      "                     [--policy.image_size int int]\n",
      "                     [--policy.base_model_path str]\n",
      "                     [--policy.tokenizer_assets_repo str]\n",
      "                     [--policy.embodiment_tag str] [--policy.tune_llm bool]\n",
      "                     [--policy.tune_visual bool]\n",
      "                     [--policy.tune_projector bool]\n",
      "                     [--policy.tune_diffusion_model bool]\n",
      "                     [--policy.lora_rank int] [--policy.lora_alpha int]\n",
      "                     [--policy.lora_dropout float]\n",
      "                     [--policy.lora_full_model bool]\n",
      "                     [--policy.warmup_ratio float] [--policy.use_bf16 bool]\n",
      "                     [--policy.video_backend str]\n",
      "                     [--policy.balance_dataset_weights bool]\n",
      "                     [--policy.balance_trajectory_weights bool]\n",
      "                     [--policy.dataset_paths [List]] [--policy.output_dir str]\n",
      "                     [--policy.save_steps int] [--policy.max_steps int]\n",
      "                     [--policy.dataloader_num_workers int]\n",
      "                     [--policy.report_to str] [--policy.resume bool]\n",
      "                     [--policy.max_action_tokens int]\n",
      "                     [--policy.text_tokenizer_name str]\n",
      "                     [--policy.action_tokenizer_name str]\n",
      "                     [--policy.temperature float]\n",
      "                     [--policy.max_decoding_steps int]\n",
      "                     [--policy.fast_skip_tokens int]\n",
      "                     [--policy.validate_action_token_prefix bool]\n",
      "                     [--policy.use_kv_cache bool]\n",
      "                     [--policy.paligemma_variant str]\n",
      "                     [--policy.action_expert_variant str]\n",
      "                     [--policy.num_inference_steps int]\n",
      "                     [--policy.time_sampling_beta_alpha float]\n",
      "                     [--policy.time_sampling_beta_beta float]\n",
      "                     [--policy.time_sampling_scale float]\n",
      "                     [--policy.time_sampling_offset float]\n",
      "                     [--policy.image_resolution int int]\n",
      "                     [--policy.gradient_checkpointing bool]\n",
      "                     [--policy.compile_model bool] [--policy.compile_mode str]\n",
      "                     [--policy.adapt_to_pi_aloha bool]\n",
      "                     [--policy.use_delta_joint_actions_aloha bool]\n",
      "                     [--policy.num_steps int] [--policy.use_cache bool]\n",
      "                     [--policy.train_expert_only bool]\n",
      "                     [--policy.train_state_proj bool]\n",
      "                     [--policy.vlm_model_name str]\n",
      "                     [--policy.load_vlm_weights bool]\n",
      "                     [--policy.add_image_special_tokens bool]\n",
      "                     [--policy.attention_mode str]\n",
      "                     [--policy.prefix_length int]\n",
      "                     [--policy.num_expert_layers int]\n",
      "                     [--policy.num_vlm_layers int]\n",
      "                     [--policy.self_attn_every_n_layers int]\n",
      "                     [--policy.expert_width_multiplier float]\n",
      "                     [--policy.min_period float] [--policy.max_period float]\n",
      "                     [--rtc_config str] [--policy.rtc_config.enabled bool]\n",
      "                     [--policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule]\n",
      "                     [--policy.rtc_config.max_guidance_weight float]\n",
      "                     [--policy.rtc_config.execution_horizon int]\n",
      "                     [--policy.rtc_config.debug bool]\n",
      "                     [--policy.rtc_config.debug_maxlen int]\n",
      "                     [--policy.n_action_repeats int] [--policy.horizon int]\n",
      "                     [--policy.q_ensemble_size int] [--policy.mlp_dim int]\n",
      "                     [--policy.use_mpc bool] [--policy.cem_iterations int]\n",
      "                     [--policy.max_std float] [--policy.min_std float]\n",
      "                     [--policy.n_gaussian_samples int]\n",
      "                     [--policy.n_pi_samples int]\n",
      "                     [--policy.uncertainty_regularizer_coeff float]\n",
      "                     [--policy.n_elites int]\n",
      "                     [--policy.elite_weighting_temperature float]\n",
      "                     [--policy.gaussian_mean_momentum float]\n",
      "                     [--policy.max_random_shift_ratio float]\n",
      "                     [--policy.reward_coeff float]\n",
      "                     [--policy.expectile_weight float]\n",
      "                     [--policy.value_coeff float]\n",
      "                     [--policy.consistency_coeff float]\n",
      "                     [--policy.advantage_scaling float]\n",
      "                     [--policy.pi_coeff float]\n",
      "                     [--policy.temporal_decay_coeff float]\n",
      "                     [--policy.target_model_momentum float]\n",
      "                     [--policy.n_action_pred_token int]\n",
      "                     [--policy.action_chunk_size int]\n",
      "                     [--policy.vision_backbone str]\n",
      "                     [--policy.crop_shape [int int]]\n",
      "                     [--policy.crop_is_random bool]\n",
      "                     [--policy.pretrained_backbone_weights [str]]\n",
      "                     [--policy.use_group_norm bool]\n",
      "                     [--policy.spatial_softmax_num_keypoints int]\n",
      "                     [--policy.n_vqvae_training_steps int]\n",
      "                     [--policy.vqvae_n_embed int]\n",
      "                     [--policy.vqvae_embedding_dim int]\n",
      "                     [--policy.vqvae_enc_hidden_dim int]\n",
      "                     [--policy.gpt_block_size int]\n",
      "                     [--policy.gpt_input_dim int]\n",
      "                     [--policy.gpt_output_dim int] [--policy.gpt_n_layer int]\n",
      "                     [--policy.gpt_n_head int] [--policy.gpt_hidden_dim int]\n",
      "                     [--policy.offset_loss_weight float]\n",
      "                     [--policy.primary_code_loss_weight float]\n",
      "                     [--policy.secondary_code_loss_weight float]\n",
      "                     [--policy.bet_softmax_temperature float]\n",
      "                     [--policy.sequentially_select bool]\n",
      "                     [--policy.optimizer_vqvae_lr float]\n",
      "                     [--policy.optimizer_vqvae_weight_decay float]\n",
      "                     [--policy.pretrained_name_or_path str]\n",
      "                     [--policy.action_tokenizer_path [str]]\n",
      "                     [--policy.prediction_mode str]\n",
      "                     [--policy.attn_implementation str]\n",
      "                     [--policy.chunk_size int] [--policy.n_action_steps int]\n",
      "                     [--policy.dtype str] [--policy.florence_config Dict]\n",
      "                     [--policy.tokenizer_name str]\n",
      "                     [--policy.tokenizer_max_length int]\n",
      "                     [--policy.tokenizer_padding_side str]\n",
      "                     [--policy.pad_language_to str] [--policy.hidden_size int]\n",
      "                     [--policy.depth int] [--policy.mlp_ratio float]\n",
      "                     [--policy.num_domains int]\n",
      "                     [--policy.len_soft_prompts int] [--policy.dim_time int]\n",
      "                     [--policy.max_len_seq int]\n",
      "                     [--policy.use_hetero_proj bool]\n",
      "                     [--policy.action_mode str]\n",
      "                     [--policy.num_denoising_steps int]\n",
      "                     [--policy.use_proprio bool] [--policy.max_action_dim int]\n",
      "                     [--policy.domain_feature_key [str]]\n",
      "                     [--policy.resize_imgs_with_padding [int int]]\n",
      "                     [--policy.num_image_views [int]]\n",
      "                     [--policy.empty_cameras int]\n",
      "                     [--policy.freeze_language_encoder bool]\n",
      "                     [--policy.train_policy_transformer bool]\n",
      "                     [--policy.train_soft_prompts bool]\n",
      "                     [--policy.optimizer_lr float]\n",
      "                     [--policy.optimizer_betas float float]\n",
      "                     [--policy.optimizer_eps float]\n",
      "                     [--policy.optimizer_weight_decay float]\n",
      "                     [--policy.optimizer_grad_clip_norm float]\n",
      "                     [--policy.optimizer_soft_prompt_lr_scale float]\n",
      "                     [--policy.optimizer_soft_prompt_warmup_lr_scale [float]]\n",
      "                     [--policy.scheduler_warmup_steps int]\n",
      "                     [--policy.scheduler_decay_steps int]\n",
      "                     [--policy.scheduler_decay_lr float]\n",
      "                     [--policy.dataset_stats [Dict]]\n",
      "                     [--policy.storage_device str]\n",
      "                     [--policy.vision_encoder_name [str]]\n",
      "                     [--policy.freeze_vision_encoder bool]\n",
      "                     [--policy.image_encoder_hidden_dim int]\n",
      "                     [--policy.shared_encoder bool]\n",
      "                     [--policy.num_discrete_actions [int]]\n",
      "                     [--policy.online_steps int]\n",
      "                     [--policy.online_buffer_capacity int]\n",
      "                     [--policy.offline_buffer_capacity int]\n",
      "                     [--policy.async_prefetch bool]\n",
      "                     [--policy.online_step_before_learning int]\n",
      "                     [--policy.policy_update_freq int]\n",
      "                     [--policy.discount float]\n",
      "                     [--policy.temperature_init float]\n",
      "                     [--policy.num_critics int]\n",
      "                     [--policy.num_subsample_critics [int]]\n",
      "                     [--policy.critic_lr float] [--policy.actor_lr float]\n",
      "                     [--policy.temperature_lr float]\n",
      "                     [--policy.critic_target_update_weight float]\n",
      "                     [--policy.utd_ratio int]\n",
      "                     [--policy.state_encoder_hidden_dim int]\n",
      "                     [--policy.target_entropy [float]]\n",
      "                     [--policy.use_backup_entropy bool]\n",
      "                     [--critic_network_kwargs str]\n",
      "                     [--policy.critic_network_kwargs.hidden_dims List]\n",
      "                     [--policy.critic_network_kwargs.activate_final bool]\n",
      "                     [--policy.critic_network_kwargs.final_activation [str]]\n",
      "                     [--actor_network_kwargs str]\n",
      "                     [--policy.actor_network_kwargs.hidden_dims List]\n",
      "                     [--policy.actor_network_kwargs.activate_final bool]\n",
      "                     [--policy_kwargs str]\n",
      "                     [--policy.policy_kwargs.use_tanh_squash bool]\n",
      "                     [--policy.policy_kwargs.std_min float]\n",
      "                     [--policy.policy_kwargs.std_max float]\n",
      "                     [--policy.policy_kwargs.init_final float]\n",
      "                     [--discrete_critic_network_kwargs str]\n",
      "                     [--policy.discrete_critic_network_kwargs.hidden_dims List]\n",
      "                     [--policy.discrete_critic_network_kwargs.activate_final bool]\n",
      "                     [--policy.discrete_critic_network_kwargs.final_activation [str]]\n",
      "                     [--actor_learner_config str]\n",
      "                     [--policy.actor_learner_config.learner_host str]\n",
      "                     [--policy.actor_learner_config.learner_port int]\n",
      "                     [--policy.actor_learner_config.policy_parameters_push_frequency int]\n",
      "                     [--policy.actor_learner_config.queue_get_timeout float]\n",
      "                     [--concurrency str] [--policy.concurrency.actor str]\n",
      "                     [--policy.concurrency.learner str]\n",
      "                     [--policy.use_torch_compile bool] [--policy.name str]\n",
      "                     [--policy.num_classes int] [--policy.latent_dim int]\n",
      "                     [--policy.image_embedding_pooling_dim int]\n",
      "                     [--policy.dropout_rate float] [--policy.model_name str]\n",
      "                     [--policy.model_type str] [--policy.num_cameras int]\n",
      "                     [--policy.learning_rate float]\n",
      "                     [--policy.weight_decay float]\n",
      "                     [--policy.grad_clip_norm float]\n",
      "                     [--policy.n_obs_steps int] [--policy.input_features dict]\n",
      "                     [--policy.output_features dict] [--policy.device [str]]\n",
      "                     [--policy.use_amp bool] [--policy.use_peft bool]\n",
      "                     [--policy.push_to_hub bool] [--policy.repo_id [str]]\n",
      "                     [--policy.private [bool]] [--policy.tags [List]]\n",
      "                     [--policy.license [str]]\n",
      "                     [--policy.pretrained_path [Path]]\n",
      "                     [--policy.annotation_mode str] [--policy.frame_gap int]\n",
      "                     [--policy.max_rewind_steps int] [--policy.image_dim int]\n",
      "                     [--policy.text_dim int] [--policy.hidden_dim int]\n",
      "                     [--policy.num_heads int] [--policy.num_layers int]\n",
      "                     [--policy.max_state_dim int]\n",
      "                     [--policy.drop_n_last_frames int]\n",
      "                     [--policy.batch_size int] [--policy.clip_batch_size int]\n",
      "                     [--policy.dropout float]\n",
      "                     [--policy.stage_loss_weight float]\n",
      "                     [--policy.rewind_probability float]\n",
      "                     [--policy.language_perturbation_probability float]\n",
      "                     [--policy.num_sparse_stages int]\n",
      "                     [--policy.sparse_subtask_names [list]]\n",
      "                     [--policy.sparse_temporal_proportions [list]]\n",
      "                     [--policy.num_dense_stages [int]]\n",
      "                     [--policy.dense_subtask_names [list]]\n",
      "                     [--policy.dense_temporal_proportions [list]]\n",
      "                     [--policy.pretrained_model_path [str]]\n",
      "                     [--policy.image_key str] [--policy.state_key str]\n",
      "                     [--policy.normalization_mapping Dict]\n",
      "                     [--output_dir [Path]] [--job_name [str]] [--resume bool]\n",
      "                     [--seed [int]] [--num_workers int] [--batch_size int]\n",
      "                     [--steps int] [--eval_freq int] [--log_freq int]\n",
      "                     [--tolerance_s float] [--save_checkpoint bool]\n",
      "                     [--save_freq int] [--use_policy_training_preset bool]\n",
      "                     [--optimizer str]\n",
      "                     [--optimizer.type {adam,adamw,sgd,xvla-adamw,multi_adam}]\n",
      "                     [--optimizer.momentum float]\n",
      "                     [--optimizer.dampening float] [--optimizer.nesterov bool]\n",
      "                     [--optimizer.betas float float] [--optimizer.eps float]\n",
      "                     [--optimizer.soft_prompt_lr_scale float]\n",
      "                     [--optimizer.soft_prompt_warmup_lr_scale [float]]\n",
      "                     [--optimizer.lr float] [--optimizer.weight_decay float]\n",
      "                     [--optimizer.grad_clip_norm float]\n",
      "                     [--optimizer.optimizer_groups Dict] [--scheduler str]\n",
      "                     [--scheduler.type {diffuser,vqbet,cosine_decay_with_warmup}]\n",
      "                     [--scheduler.name str]\n",
      "                     [--scheduler.num_vqvae_training_steps int]\n",
      "                     [--scheduler.num_cycles float]\n",
      "                     [--scheduler.num_warmup_steps int]\n",
      "                     [--scheduler.num_decay_steps int]\n",
      "                     [--scheduler.peak_lr float] [--scheduler.decay_lr float]\n",
      "                     [--eval str] [--eval.n_episodes int]\n",
      "                     [--eval.batch_size int] [--eval.use_async_envs bool]\n",
      "                     [--wandb str] [--wandb.enable bool]\n",
      "                     [--wandb.disable_artifact bool] [--wandb.project str]\n",
      "                     [--wandb.entity [str]] [--wandb.notes [str]]\n",
      "                     [--wandb.run_id [str]] [--wandb.mode [str]] [--peft str]\n",
      "                     [--peft.target_modules [List|str]]\n",
      "                     [--peft.full_training_modules [List]]\n",
      "                     [--peft.method_type str] [--peft.init_type [str]]\n",
      "                     [--peft.r int] [--use_rabc bool]\n",
      "                     [--rabc_progress_path [str]] [--rabc_kappa float]\n",
      "                     [--rabc_epsilon float] [--rabc_head_mode [str]]\n",
      "                     [--rename_map Dict]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --config_path str     Path for a config file to parse with draccus (default:\n",
      "                        None)\n",
      "  --dataset str         Config file for dataset (default: None)\n",
      "  --image_transforms str\n",
      "                        Config file for image_transforms (default: None)\n",
      "  --env str             Config file for env (default: None)\n",
      "  --robot str           Config file for robot (default: None)\n",
      "  --teleop str          Config file for teleop (default: None)\n",
      "  --processor str       Config file for processor (default: None)\n",
      "  --observation str     Config file for observation (default: None)\n",
      "  --image_preprocessing str\n",
      "                        Config file for image_preprocessing (default: None)\n",
      "  --gripper str         Config file for gripper (default: None)\n",
      "  --reset str           Config file for reset (default: None)\n",
      "  --inverse_kinematics str\n",
      "                        Config file for inverse_kinematics (default: None)\n",
      "  --reward_classifier str\n",
      "                        Config file for reward_classifier (default: None)\n",
      "  --policy str          Config file for policy (default: None)\n",
      "  --rtc_config str      Config file for rtc_config (default: None)\n",
      "  --rtc_config str      Config file for rtc_config (default: None)\n",
      "  --rtc_config str      Config file for rtc_config (default: None)\n",
      "  --rtc_config str      Config file for rtc_config (default: None)\n",
      "  --critic_network_kwargs str\n",
      "                        Config file for critic_network_kwargs (default: None)\n",
      "  --actor_network_kwargs str\n",
      "                        Config file for actor_network_kwargs (default: None)\n",
      "  --policy_kwargs str   Config file for policy_kwargs (default: None)\n",
      "  --discrete_critic_network_kwargs str\n",
      "                        Config file for discrete_critic_network_kwargs\n",
      "                        (default: None)\n",
      "  --actor_learner_config str\n",
      "                        Config file for actor_learner_config (default: None)\n",
      "  --concurrency str     Config file for concurrency (default: None)\n",
      "  --optimizer str       Config file for optimizer (default: None)\n",
      "  --scheduler str       Config file for scheduler (default: None)\n",
      "  --eval str            Config file for eval (default: None)\n",
      "  --wandb str           Config file for wandb (default: None)\n",
      "  --peft str            Config file for peft (default: None)\n",
      "\n",
      "TrainPipelineConfig:\n",
      "\n",
      "  --output_dir [Path]   Set `dir` to where you would like to save all of the\n",
      "                        run outputs. If you run another training session with\n",
      "                        the same value for `dir` its contents will be\n",
      "                        overwritten unless you set `resume` to true. (default:\n",
      "                        None)\n",
      "  --job_name [str]\n",
      "  --resume bool         Set `resume` to true to resume a previous run. In\n",
      "                        order for this to work, you will need to make sure\n",
      "                        `dir` is the directory of an existing run with at\n",
      "                        least one checkpoint in it. Note that when resuming a\n",
      "                        run, the default behavior is to use the configuration\n",
      "                        from the checkpoint, regardless of what's provided\n",
      "                        with the training command at the time of resumption.\n",
      "                        (default: False)\n",
      "  --seed [int]          `seed` is used for training (eg: model initialization,\n",
      "                        dataset shuffling) AND for the evaluation\n",
      "                        environments. (default: 1000)\n",
      "  --num_workers int     Number of workers for the dataloader. (default: 4)\n",
      "  --batch_size int      \n",
      "  --steps int           \n",
      "  --eval_freq int       \n",
      "  --log_freq int        \n",
      "  --tolerance_s float   \n",
      "  --save_checkpoint bool\n",
      "  --save_freq int       Checkpoint is saved every `save_freq` training\n",
      "                        iterations and after the last training step. (default:\n",
      "                        20000)\n",
      "  --use_policy_training_preset bool\n",
      "  --use_rabc bool       Enable reward-weighted training (default: False)\n",
      "  --rabc_progress_path [str]\n",
      "                        Path to precomputed SARM progress parquet file\n",
      "                        (default: None)\n",
      "  --rabc_kappa float    Hard threshold for high-quality samples (default:\n",
      "                        0.01)\n",
      "  --rabc_epsilon float  Small constant for numerical stability (default:\n",
      "                        1e-06)\n",
      "  --rabc_head_mode [str]\n",
      "                        For dual-head models: \"sparse\" or \"dense\" (default:\n",
      "                        sparse)\n",
      "  --rename_map Dict     Rename map for the observation to override the image\n",
      "                        and state keys (default: {})\n",
      "\n",
      "DatasetConfig ['dataset']:\n",
      "\n",
      "  --dataset.repo_id str\n",
      "                        You may provide a list of datasets here. `train.py`\n",
      "                        creates them all and concatenates them. Note: only\n",
      "                        data keys common between the datasets are kept. Each\n",
      "                        dataset gets and additional transform that inserts the\n",
      "                        \"dataset_index\" into the returned item. The index\n",
      "                        mapping is made according to the order in which the\n",
      "                        datasets are provided. (default: None)\n",
      "  --dataset.root [str]  Root directory where the dataset will be stored (e.g.\n",
      "                        'dataset/path'). (default: None)\n",
      "  --dataset.episodes [List]\n",
      "  --dataset.revision [str]\n",
      "  --dataset.use_imagenet_stats bool\n",
      "  --dataset.video_backend str\n",
      "  --dataset.streaming bool\n",
      "\n",
      "ImageTransformsConfig ['dataset.image_transforms']:\n",
      "  \n",
      "      These transforms are all using standard torchvision.transforms.v2\n",
      "      You can find out how these transformations affect images here:\n",
      "      https://pytorch.org/vision/0.18/auto_examples/transforms/plot_transforms_illustrations.html\n",
      "      We use a custom RandomSubsetApply container to sample them.\n",
      "      \n",
      "\n",
      "  --dataset.image_transforms.enable bool\n",
      "                        Set this flag to `true` to enable transforms during\n",
      "                        training (default: False)\n",
      "  --dataset.image_transforms.max_num_transforms int\n",
      "                        This is the maximum number of transforms (sampled from\n",
      "                        these below) that will be applied to each frame. It's\n",
      "                        an integer in the interval [1,\n",
      "                        number_of_available_transforms]. (default: 3)\n",
      "  --dataset.image_transforms.random_order bool\n",
      "                        By default, transforms are applied in Torchvision's\n",
      "                        suggested order (shown below). Set this to True to\n",
      "                        apply them in a random order. (default: False)\n",
      "  --dataset.image_transforms.tfs Dict\n",
      "\n",
      "Optional ['env']:\n",
      "\n",
      "EnvConfig ['env']:\n",
      "\n",
      "  --env.type {aloha,pusht,gym_manipulator,libero,metaworld,isaaclab_arena}\n",
      "                        Which type of EnvConfig ['env'] to use (default: None)\n",
      "\n",
      "AlohaEnv ['env']:\n",
      "\n",
      "  --env.task [str]      \n",
      "  --env.fps int         \n",
      "  --env.features Dict   \n",
      "  --env.features_map Dict\n",
      "  --env.max_parallel_tasks int\n",
      "  --env.disable_env_checker bool\n",
      "  --env.episode_length int\n",
      "  --env.obs_type str    \n",
      "  --env.observation_height int\n",
      "  --env.observation_width int\n",
      "  --env.render_mode str\n",
      "\n",
      "PushtEnv ['env']:\n",
      "\n",
      "  --env.task [str]      \n",
      "  --env.fps int         \n",
      "  --env.features Dict   \n",
      "  --env.features_map Dict\n",
      "  --env.max_parallel_tasks int\n",
      "  --env.disable_env_checker bool\n",
      "  --env.episode_length int\n",
      "  --env.obs_type str    \n",
      "  --env.render_mode str\n",
      "  --env.visualization_width int\n",
      "  --env.visualization_height int\n",
      "  --env.observation_height int\n",
      "  --env.observation_width int\n",
      "\n",
      "HILSerlRobotEnvConfig ['env']:\n",
      "  Configuration for the HILSerlRobotEnv environment.\n",
      "\n",
      "  --env.task [str]\n",
      "  --env.fps int         \n",
      "  --env.features Dict   \n",
      "  --env.features_map Dict\n",
      "  --env.max_parallel_tasks int\n",
      "  --env.disable_env_checker bool\n",
      "  --env.name str        \n",
      "\n",
      "Optional ['env.robot']:\n",
      "\n",
      "RobotConfig ['env.robot']:\n",
      "\n",
      "  --env.robot.type {}   Which type of RobotConfig ['env.robot'] to use\n",
      "                        (default: None)\n",
      "\n",
      "Optional ['env.teleop']:\n",
      "\n",
      "TeleoperatorConfig ['env.teleop']:\n",
      "\n",
      "  --env.teleop.type {}  Which type of TeleoperatorConfig ['env.teleop'] to use\n",
      "                        (default: None)\n",
      "\n",
      "HILSerlProcessorConfig ['env.processor']:\n",
      "  Configuration for environment processing pipeline.\n",
      "\n",
      "  --env.processor.control_mode str\n",
      "  --env.processor.max_gripper_pos [float]\n",
      "\n",
      "Optional ['env.processor.observation']:\n",
      "\n",
      "ObservationConfig ['env.processor.observation']:\n",
      "  Configuration for observation processing.\n",
      "\n",
      "  --env.processor.observation.add_joint_velocity_to_observation bool\n",
      "  --env.processor.observation.add_current_to_observation bool\n",
      "  --env.processor.observation.display_cameras bool\n",
      "\n",
      "Optional ['env.processor.image_preprocessing']:\n",
      "\n",
      "ImagePreprocessingConfig ['env.processor.image_preprocessing']:\n",
      "\n",
      "  --env.processor.image_preprocessing.crop_params_dict [Dict]\n",
      "  --env.processor.image_preprocessing.resize_size [int int]\n",
      "\n",
      "Optional ['env.processor.gripper']:\n",
      "\n",
      "GripperConfig ['env.processor.gripper']:\n",
      "  Configuration for gripper control and penalties.\n",
      "\n",
      "  --env.processor.gripper.use_gripper bool\n",
      "  --env.processor.gripper.gripper_penalty float\n",
      "\n",
      "Optional ['env.processor.reset']:\n",
      "\n",
      "ResetConfig ['env.processor.reset']:\n",
      "  Configuration for environment reset behavior.\n",
      "\n",
      "  --env.processor.reset.fixed_reset_joint_positions [Any]\n",
      "  --env.processor.reset.reset_time_s float\n",
      "  --env.processor.reset.control_time_s float\n",
      "  --env.processor.reset.terminate_on_success bool\n",
      "\n",
      "Optional ['env.processor.inverse_kinematics']:\n",
      "\n",
      "InverseKinematicsConfig ['env.processor.inverse_kinematics']:\n",
      "  Configuration for inverse kinematics processing.\n",
      "\n",
      "  --env.processor.inverse_kinematics.urdf_path [str]\n",
      "  --env.processor.inverse_kinematics.target_frame_name [str]\n",
      "  --env.processor.inverse_kinematics.end_effector_bounds [Dict]\n",
      "  --env.processor.inverse_kinematics.end_effector_step_sizes [Dict]\n",
      "\n",
      "Optional ['env.processor.reward_classifier']:\n",
      "\n",
      "RewardClassifierConfig ['env.processor.reward_classifier']:\n",
      "  Configuration for reward classification.\n",
      "\n",
      "  --env.processor.reward_classifier.pretrained_path [str]\n",
      "  --env.processor.reward_classifier.success_threshold float\n",
      "  --env.processor.reward_classifier.success_reward float\n",
      "\n",
      "LiberoEnv ['env']:\n",
      "\n",
      "  --env.task str        can also choose libero_spatial, libero_object, etc.\n",
      "                        (default: libero_10)\n",
      "  --env.fps int         \n",
      "  --env.features Dict   \n",
      "  --env.features_map Dict\n",
      "  --env.max_parallel_tasks int\n",
      "  --env.disable_env_checker bool\n",
      "  --env.episode_length [int]\n",
      "  --env.obs_type str    \n",
      "  --env.render_mode str\n",
      "  --env.camera_name str\n",
      "  --env.init_states bool\n",
      "  --env.camera_name_mapping [Dict]\n",
      "  --env.observation_height int\n",
      "  --env.observation_width int\n",
      "  --env.control_mode str\n",
      "                        or \"absolute\" (default: relative)\n",
      "\n",
      "MetaworldEnv ['env']:\n",
      "\n",
      "  --env.task str        add all tasks (default: metaworld-push-v2)\n",
      "  --env.fps int         \n",
      "  --env.features Dict   \n",
      "  --env.features_map Dict\n",
      "  --env.max_parallel_tasks int\n",
      "  --env.disable_env_checker bool\n",
      "  --env.episode_length int\n",
      "  --env.obs_type str    \n",
      "  --env.render_mode str\n",
      "  --env.multitask_eval bool\n",
      "\n",
      "IsaaclabArenaEnv ['env']:\n",
      "\n",
      "  --env.task [str]      \n",
      "  --env.fps int         \n",
      "  --env.features Dict   \n",
      "  --env.features_map Dict\n",
      "  --env.max_parallel_tasks int\n",
      "  --env.disable_env_checker bool\n",
      "  --env.hub_path str    \n",
      "  --env.episode_length int\n",
      "  --env.num_envs int    \n",
      "  --env.embodiment [str]\n",
      "  --env.object [str]    \n",
      "  --env.mimic bool      \n",
      "  --env.teleop_device [str]\n",
      "  --env.seed [int]      \n",
      "  --env.device [str]    \n",
      "  --env.disable_fabric bool\n",
      "  --env.enable_cameras bool\n",
      "  --env.headless bool   \n",
      "  --env.enable_pinocchio bool\n",
      "  --env.environment [str]\n",
      "  --env.state_dim int   \n",
      "  --env.action_dim int  \n",
      "  --env.camera_height int\n",
      "  --env.camera_width int\n",
      "  --env.video bool      \n",
      "  --env.video_length int\n",
      "  --env.video_interval int\n",
      "  --env.state_keys str  Comma-separated keys, e.g.,\n",
      "                        \"robot_joint_pos,left_eef_pos\" (default:\n",
      "                        robot_joint_pos)\n",
      "  --env.camera_keys [str]\n",
      "                        Comma-separated keys, e.g.,\n",
      "                        \"robot_pov_cam_rgb,front_cam_rgb\" Set to None or \"\"\n",
      "                        for environments without cameras (default: None)\n",
      "  --env.kwargs [dict]\n",
      "\n",
      "Optional ['policy']:\n",
      "\n",
      "PreTrainedConfig ['policy']:\n",
      "  \n",
      "      Base configuration class for policy models.\n",
      "  \n",
      "      Args:\n",
      "          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the\n",
      "              current step and additional steps going back).\n",
      "          input_shapes: A dictionary defining the shapes of the input data for the policy.\n",
      "          output_shapes: A dictionary defining the shapes of the output data for the policy.\n",
      "          input_normalization_modes: A dictionary with key representing the modality and the value specifies the\n",
      "              normalization mode to apply.\n",
      "          output_normalization_modes: Similar dictionary as `input_normalization_modes`, but to unnormalize to\n",
      "              the original scale.\n",
      "      \n",
      "\n",
      "  --policy.type {act,diffusion,groot,pi0,pi0_fast,pi05,smolvla,tdmpc,vqbet,wall_x,xvla,sac,reward_classifier,sarm}\n",
      "                        Which type of PreTrainedConfig ['policy'] to use\n",
      "                        (default: None)\n",
      "\n",
      "ACTConfig ['policy']:\n",
      "  Configuration class for the Action Chunking Transformers policy.\n",
      "  \n",
      "      Defaults are configured for training on bimanual Aloha tasks like \"insertion\" or \"transfer\".\n",
      "  \n",
      "      The parameters you will most likely need to change are the ones which depend on the environment / sensors.\n",
      "      Those are: `input_shapes` and 'output_shapes`.\n",
      "  \n",
      "      Notes on the inputs and outputs:\n",
      "          - Either:\n",
      "              - At least one key starting with \"observation.image is required as an input.\n",
      "                AND/OR\n",
      "              - The key \"observation.environment_state\" is required as input.\n",
      "          - If there are multiple keys beginning with \"observation.images.\" they are treated as multiple camera\n",
      "            views. Right now we only support all images having the same shape.\n",
      "          - May optionally work without an \"observation.state\" key for the proprioceptive robot state.\n",
      "          - \"action\" is required as an output key.\n",
      "  \n",
      "      Args:\n",
      "          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the\n",
      "              current step and additional steps going back).\n",
      "          chunk_size: The size of the action prediction \"chunks\" in units of environment steps.\n",
      "          n_action_steps: The number of action steps to run in the environment for one invocation of the policy.\n",
      "              This should be no greater than the chunk size. For example, if the chunk size size 100, you may\n",
      "              set this to 50. This would mean that the model predicts 100 steps worth of actions, runs 50 in the\n",
      "              environment, and throws the other 50 out.\n",
      "          input_shapes: A dictionary defining the shapes of the input data for the policy. The key represents\n",
      "              the input data name, and the value is a list indicating the dimensions of the corresponding data.\n",
      "              For example, \"observation.image\" refers to an input from a camera with dimensions [3, 96, 96],\n",
      "              indicating it has three color channels and 96x96 resolution. Importantly, `input_shapes` doesn't\n",
      "              include batch dimension or temporal dimension.\n",
      "          output_shapes: A dictionary defining the shapes of the output data for the policy. The key represents\n",
      "              the output data name, and the value is a list indicating the dimensions of the corresponding data.\n",
      "              For example, \"action\" refers to an output shape of [14], indicating 14-dimensional actions.\n",
      "              Importantly, `output_shapes` doesn't include batch dimension or temporal dimension.\n",
      "          input_normalization_modes: A dictionary with key representing the modality (e.g. \"observation.state\"),\n",
      "              and the value specifies the normalization mode to apply. The two available modes are \"mean_std\"\n",
      "              which subtracts the mean and divides by the standard deviation and \"min_max\" which rescale in a\n",
      "              [-1, 1] range.\n",
      "          output_normalization_modes: Similar dictionary as `normalize_input_modes`, but to unnormalize to the\n",
      "              original scale. Note that this is also used for normalizing the training targets.\n",
      "          vision_backbone: Name of the torchvision resnet backbone to use for encoding images.\n",
      "          pretrained_backbone_weights: Pretrained weights from torchvision to initialize the backbone.\n",
      "              `None` means no pretrained weights.\n",
      "          replace_final_stride_with_dilation: Whether to replace the ResNet's final 2x2 stride with a dilated\n",
      "              convolution.\n",
      "          pre_norm: Whether to use \"pre-norm\" in the transformer blocks.\n",
      "          dim_model: The transformer blocks' main hidden dimension.\n",
      "          n_heads: The number of heads to use in the transformer blocks' multi-head attention.\n",
      "          dim_feedforward: The dimension to expand the transformer's hidden dimension to in the feed-forward\n",
      "              layers.\n",
      "          feedforward_activation: The activation to use in the transformer block's feed-forward layers.\n",
      "          n_encoder_layers: The number of transformer layers to use for the transformer encoder.\n",
      "          n_decoder_layers: The number of transformer layers to use for the transformer decoder.\n",
      "          use_vae: Whether to use a variational objective during training. This introduces another transformer\n",
      "              which is used as the VAE's encoder (not to be confused with the transformer encoder - see\n",
      "              documentation in the policy class).\n",
      "          latent_dim: The VAE's latent dimension.\n",
      "          n_vae_encoder_layers: The number of transformer layers to use for the VAE's encoder.\n",
      "          temporal_ensemble_coeff: Coefficient for the exponential weighting scheme to apply for temporal\n",
      "              ensembling. Defaults to None which means temporal ensembling is not used. `n_action_steps` must be\n",
      "              1 when using this feature, as inference needs to happen at every step to form an ensemble. For\n",
      "              more information on how ensembling works, please see `ACTTemporalEnsembler`.\n",
      "          dropout: Dropout to use in the transformer layers (see code for details).\n",
      "          kl_weight: The weight to use for the KL-divergence component of the loss if the variational objective\n",
      "              is enabled. Loss is then calculated as: `reconstruction_loss + kl_weight * kld_loss`.\n",
      "      \n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.chunk_size int\n",
      "  --policy.n_action_steps int\n",
      "  --policy.normalization_mapping Dict\n",
      "  --policy.vision_backbone str\n",
      "  --policy.pretrained_backbone_weights [str]\n",
      "  --policy.replace_final_stride_with_dilation int\n",
      "  --policy.pre_norm bool\n",
      "  --policy.dim_model int\n",
      "  --policy.n_heads int  \n",
      "  --policy.dim_feedforward int\n",
      "  --policy.feedforward_activation str\n",
      "  --policy.n_encoder_layers int\n",
      "  --policy.n_decoder_layers int\n",
      "  --policy.use_vae bool\n",
      "  --policy.latent_dim int\n",
      "  --policy.n_vae_encoder_layers int\n",
      "  --policy.temporal_ensemble_coeff [float]\n",
      "  --policy.dropout float\n",
      "  --policy.kl_weight float\n",
      "  --policy.optimizer_lr float\n",
      "                        Training preset (default: 1e-05)\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.optimizer_lr_backbone float\n",
      "\n",
      "DiffusionConfig ['policy']:\n",
      "  Configuration class for DiffusionPolicy.\n",
      "  \n",
      "      Defaults are configured for training with PushT providing proprioceptive and single camera observations.\n",
      "  \n",
      "      The parameters you will most likely need to change are the ones which depend on the environment / sensors.\n",
      "      Those are: `input_shapes` and `output_shapes`.\n",
      "  \n",
      "      Notes on the inputs and outputs:\n",
      "          - \"observation.state\" is required as an input key.\n",
      "          - Either:\n",
      "              - At least one key starting with \"observation.image is required as an input.\n",
      "                AND/OR\n",
      "              - The key \"observation.environment_state\" is required as input.\n",
      "          - If there are multiple keys beginning with \"observation.image\" they are treated as multiple camera\n",
      "            views. Right now we only support all images having the same shape.\n",
      "          - \"action\" is required as an output key.\n",
      "  \n",
      "      Args:\n",
      "          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the\n",
      "              current step and additional steps going back).\n",
      "          horizon: Diffusion model action prediction size as detailed in `DiffusionPolicy.select_action`.\n",
      "          n_action_steps: The number of action steps to run in the environment for one invocation of the policy.\n",
      "              See `DiffusionPolicy.select_action` for more details.\n",
      "          input_shapes: A dictionary defining the shapes of the input data for the policy. The key represents\n",
      "              the input data name, and the value is a list indicating the dimensions of the corresponding data.\n",
      "              For example, \"observation.image\" refers to an input from a camera with dimensions [3, 96, 96],\n",
      "              indicating it has three color channels and 96x96 resolution. Importantly, `input_shapes` doesn't\n",
      "              include batch dimension or temporal dimension.\n",
      "          output_shapes: A dictionary defining the shapes of the output data for the policy. The key represents\n",
      "              the output data name, and the value is a list indicating the dimensions of the corresponding data.\n",
      "              For example, \"action\" refers to an output shape of [14], indicating 14-dimensional actions.\n",
      "              Importantly, `output_shapes` doesn't include batch dimension or temporal dimension.\n",
      "          input_normalization_modes: A dictionary with key representing the modality (e.g. \"observation.state\"),\n",
      "              and the value specifies the normalization mode to apply. The two available modes are \"mean_std\"\n",
      "              which subtracts the mean and divides by the standard deviation and \"min_max\" which rescale in a\n",
      "              [-1, 1] range.\n",
      "          output_normalization_modes: Similar dictionary as `normalize_input_modes`, but to unnormalize to the\n",
      "              original scale. Note that this is also used for normalizing the training targets.\n",
      "          vision_backbone: Name of the torchvision resnet backbone to use for encoding images.\n",
      "          crop_shape: (H, W) shape to crop images to as a preprocessing step for the vision backbone. Must fit\n",
      "              within the image size. If None, no cropping is done.\n",
      "          crop_is_random: Whether the crop should be random at training time (it's always a center crop in eval\n",
      "              mode).\n",
      "          pretrained_backbone_weights: Pretrained weights from torchvision to initialize the backbone.\n",
      "              `None` means no pretrained weights.\n",
      "          use_group_norm: Whether to replace batch normalization with group normalization in the backbone.\n",
      "              The group sizes are set to be about 16 (to be precise, feature_dim // 16).\n",
      "          spatial_softmax_num_keypoints: Number of keypoints for SpatialSoftmax.\n",
      "          use_separate_rgb_encoders_per_camera: Whether to use a separate RGB encoder for each camera view.\n",
      "          down_dims: Feature dimension for each stage of temporal downsampling in the diffusion modeling Unet.\n",
      "              You may provide a variable number of dimensions, therefore also controlling the degree of\n",
      "              downsampling.\n",
      "          kernel_size: The convolutional kernel size of the diffusion modeling Unet.\n",
      "          n_groups: Number of groups used in the group norm of the Unet's convolutional blocks.\n",
      "          diffusion_step_embed_dim: The Unet is conditioned on the diffusion timestep via a small non-linear\n",
      "              network. This is the output dimension of that network, i.e., the embedding dimension.\n",
      "          use_film_scale_modulation: FiLM (https://huggingface.co/papers/1709.07871) is used for the Unet conditioning.\n",
      "              Bias modulation is used be default, while this parameter indicates whether to also use scale\n",
      "              modulation.\n",
      "          noise_scheduler_type: Name of the noise scheduler to use. Supported options: [\"DDPM\", \"DDIM\"].\n",
      "          num_train_timesteps: Number of diffusion steps for the forward diffusion schedule.\n",
      "          beta_schedule: Name of the diffusion beta schedule as per DDPMScheduler from Hugging Face diffusers.\n",
      "          beta_start: Beta value for the first forward-diffusion step.\n",
      "          beta_end: Beta value for the last forward-diffusion step.\n",
      "          prediction_type: The type of prediction that the diffusion modeling Unet makes. Choose from \"epsilon\"\n",
      "              or \"sample\". These have equivalent outcomes from a latent variable modeling perspective, but\n",
      "              \"epsilon\" has been shown to work better in many deep neural network settings.\n",
      "          clip_sample: Whether to clip the sample to [-`clip_sample_range`, +`clip_sample_range`] for each\n",
      "              denoising step at inference time. WARNING: you will need to make sure your action-space is\n",
      "              normalized to fit within this range.\n",
      "          clip_sample_range: The magnitude of the clipping range as described above.\n",
      "          num_inference_steps: Number of reverse diffusion steps to use at inference time (steps are evenly\n",
      "              spaced). If not provided, this defaults to be the same as `num_train_timesteps`.\n",
      "          do_mask_loss_for_padding: Whether to mask the loss when there are copy-padded actions. See\n",
      "              `LeRobotDataset` and `load_previous_and_future_frames` for more information. Note, this defaults\n",
      "              to False as the original Diffusion Policy implementation does the same.\n",
      "      \n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.horizon int  \n",
      "  --policy.n_action_steps int\n",
      "  --policy.normalization_mapping Dict\n",
      "  --policy.drop_n_last_frames int\n",
      "                        horizon - n_action_steps - n_obs_steps + 1 (default:\n",
      "                        7)\n",
      "  --policy.vision_backbone str\n",
      "  --policy.crop_shape [int int]\n",
      "  --policy.crop_is_random bool\n",
      "  --policy.pretrained_backbone_weights [str]\n",
      "  --policy.use_group_norm bool\n",
      "  --policy.spatial_softmax_num_keypoints int\n",
      "  --policy.use_separate_rgb_encoder_per_camera bool\n",
      "  --policy.down_dims int [int, ...]\n",
      "  --policy.kernel_size int\n",
      "  --policy.n_groups int\n",
      "  --policy.diffusion_step_embed_dim int\n",
      "  --policy.use_film_scale_modulation bool\n",
      "  --policy.noise_scheduler_type str\n",
      "                        Noise scheduler. (default: DDPM)\n",
      "  --policy.num_train_timesteps int\n",
      "  --policy.beta_schedule str\n",
      "  --policy.beta_start float\n",
      "  --policy.beta_end float\n",
      "  --policy.prediction_type str\n",
      "  --policy.clip_sample bool\n",
      "  --policy.clip_sample_range float\n",
      "  --policy.num_inference_steps [int]\n",
      "  --policy.do_mask_loss_for_padding bool\n",
      "  --policy.optimizer_lr float\n",
      "                        Training presets (default: 0.0001)\n",
      "  --policy.optimizer_betas Any\n",
      "  --policy.optimizer_eps float\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.scheduler_name str\n",
      "  --policy.scheduler_warmup_steps int\n",
      "\n",
      "GrootConfig ['policy']:\n",
      "  Configuration for Groot policy wrapper.\n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "                        Basic policy settings (default: 1)\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.chunk_size int\n",
      "  --policy.n_action_steps int\n",
      "  --policy.max_state_dim int\n",
      "                        Dimension settings (must match pretrained GR00T model\n",
      "                        expectations) Maximum state dimension. Shorter states\n",
      "                        will be zero-padded. (default: 64)\n",
      "  --policy.max_action_dim int\n",
      "                        Maximum action dimension. Shorter actions will be\n",
      "                        zero-padded. (default: 32)\n",
      "  --policy.normalization_mapping Dict\n",
      "                        Normalization (start with identity, adjust as needed)\n",
      "                        (default: {'VISUAL': <NormalizationMode.IDENTITY:\n",
      "                        'IDENTITY'>, 'STATE': <NormalizationMode.MEAN_STD:\n",
      "                        'MEAN_STD'>, 'ACTION': <NormalizationMode.MEAN_STD:\n",
      "                        'MEAN_STD'>})\n",
      "  --policy.image_size int int\n",
      "                        Image preprocessing (adjust to match Groot's expected\n",
      "                        input) (default: (224, 224))\n",
      "  --policy.base_model_path str\n",
      "                        Groot-specific model parameters (from\n",
      "                        groot_finetune_script.py) Path or HuggingFace model ID\n",
      "                        for the base Groot model (default:\n",
      "                        nvidia/GR00T-N1.5-3B)\n",
      "  --policy.tokenizer_assets_repo str\n",
      "                        HF repo ID (or local path) that hosts vocab.json and\n",
      "                        merges.txt for Eagle tokenizer. (default:\n",
      "                        lerobot/eagle2hg-processor-groot-n1p5)\n",
      "  --policy.embodiment_tag str\n",
      "                        Embodiment tag to use for training (e.g.\n",
      "                        'new_embodiment', 'gr1') (default: new_embodiment)\n",
      "  --policy.tune_llm bool\n",
      "                        Fine-tuning control arguments Whether to fine-tune the\n",
      "                        llm backbone (default: False)\n",
      "  --policy.tune_visual bool\n",
      "                        Whether to fine-tune the vision tower (default: False)\n",
      "  --policy.tune_projector bool\n",
      "                        Whether to fine-tune the projector (default: True)\n",
      "  --policy.tune_diffusion_model bool\n",
      "                        Whether to fine-tune the diffusion model (default:\n",
      "                        True)\n",
      "  --policy.lora_rank int\n",
      "                        LoRA parameters (from groot_finetune_script.py) Rank\n",
      "                        for the LORA model. If 0, no LORA will be used.\n",
      "                        (default: 0)\n",
      "  --policy.lora_alpha int\n",
      "                        Alpha value for the LORA model (default: 16)\n",
      "  --policy.lora_dropout float\n",
      "                        Dropout rate for the LORA model (default: 0.1)\n",
      "  --policy.lora_full_model bool\n",
      "                        Whether to use the full model for LORA (default:\n",
      "                        False)\n",
      "  --policy.optimizer_lr float\n",
      "                        Training parameters (matching\n",
      "                        groot_finetune_script.py) (default: 0.0001)\n",
      "  --policy.optimizer_betas float float\n",
      "  --policy.optimizer_eps float\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.warmup_ratio float\n",
      "  --policy.use_bf16 bool\n",
      "  --policy.video_backend str\n",
      "                        Dataset parameters Video backend to use for training\n",
      "                        ('decord' or 'torchvision_av') (default: decord)\n",
      "  --policy.balance_dataset_weights bool\n",
      "                        Whether to balance dataset weights in mixture datasets\n",
      "                        (default: True)\n",
      "  --policy.balance_trajectory_weights bool\n",
      "                        Whether to sample trajectories weighted by their\n",
      "                        length (default: True)\n",
      "  --policy.dataset_paths [List]\n",
      "                        Optional dataset paths for delegating training to\n",
      "                        Isaac-GR00T runner (default: None)\n",
      "  --policy.output_dir str\n",
      "  --policy.save_steps int\n",
      "  --policy.max_steps int\n",
      "  --policy.batch_size int\n",
      "  --policy.dataloader_num_workers int\n",
      "  --policy.report_to str\n",
      "  --policy.resume bool  \n",
      "\n",
      "PI0Config ['policy']:\n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        Device to use for the model (None = auto-detect)\n",
      "                        (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.paligemma_variant str\n",
      "  --policy.action_expert_variant str\n",
      "  --policy.dtype str    Options: \"bfloat16\", \"float32\" (default: float32)\n",
      "  --policy.chunk_size int\n",
      "                        Number of action steps to predict, in openpi called\n",
      "                        \"action_horizon\" (default: 50)\n",
      "  --policy.n_action_steps int\n",
      "                        Number of action steps to execute (default: 50)\n",
      "  --policy.max_state_dim int\n",
      "                        Shorter state and action vectors will be padded to\n",
      "                        these dimensions (default: 32)\n",
      "  --policy.max_action_dim int\n",
      "  --policy.num_inference_steps int\n",
      "                        Number of denoising steps during inference (default:\n",
      "                        10)\n",
      "  --policy.time_sampling_beta_alpha float\n",
      "  --policy.time_sampling_beta_beta float\n",
      "  --policy.time_sampling_scale float\n",
      "  --policy.time_sampling_offset float\n",
      "  --policy.min_period float\n",
      "  --policy.max_period float\n",
      "  --policy.image_resolution int int\n",
      "  --policy.empty_cameras int\n",
      "                        see openpi `preprocessing_pytorch.py` Add empty\n",
      "                        images. Used to add empty cameras when no image\n",
      "                        features are present. (default: 0)\n",
      "  --policy.normalization_mapping Dict\n",
      "                        Normalization (default: {'VISUAL':\n",
      "                        <NormalizationMode.IDENTITY: 'IDENTITY'>, 'STATE':\n",
      "                        <NormalizationMode.MEAN_STD: 'MEAN_STD'>, 'ACTION':\n",
      "                        <NormalizationMode.MEAN_STD: 'MEAN_STD'>})\n",
      "  --policy.gradient_checkpointing bool\n",
      "                        Enable gradient checkpointing for memory optimization\n",
      "                        (default: False)\n",
      "  --policy.compile_model bool\n",
      "                        Whether to use torch.compile for model optimization\n",
      "                        (default: False)\n",
      "  --policy.compile_mode str\n",
      "                        Torch compile mode (default: max-autotune)\n",
      "  --policy.freeze_vision_encoder bool\n",
      "                        Freeze only the vision encoder (default: False)\n",
      "  --policy.train_expert_only bool\n",
      "                        Freeze entire VLM, train only action expert and\n",
      "                        projections (default: False)\n",
      "  --policy.optimizer_lr float\n",
      "                        see openpi `CosineDecaySchedule: peak_lr` (default:\n",
      "                        2.5e-05)\n",
      "  --policy.optimizer_betas float float\n",
      "  --policy.optimizer_eps float\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.optimizer_grad_clip_norm float\n",
      "  --policy.scheduler_warmup_steps int\n",
      "                        Scheduler settings: see openpi `CosineDecaySchedule`\n",
      "                        Note: These will auto-scale if --steps <\n",
      "                        scheduler_decay_steps For example, --steps=3000 will\n",
      "                        scale warmup to 100 and decay to 3000 (default: 1000)\n",
      "  --policy.scheduler_decay_steps int\n",
      "  --policy.scheduler_decay_lr float\n",
      "  --policy.tokenizer_max_length int\n",
      "                        see openpi `__post_init__` (default: 48)\n",
      "\n",
      "Optional ['policy.rtc_config']:\n",
      "  Real-Time Chunking (RTC) configuration\n",
      "\n",
      "RTCConfig ['policy.rtc_config']:\n",
      "  Real-Time Chunking (RTC) configuration\n",
      "\n",
      "  --policy.rtc_config.enabled bool\n",
      "                        Infrastructure (default: False)\n",
      "  --policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule\n",
      "                        Core RTC settings Todo change to exp (default:\n",
      "                        RTCAttentionSchedule.LINEAR)\n",
      "  --policy.rtc_config.max_guidance_weight float\n",
      "  --policy.rtc_config.execution_horizon int\n",
      "  --policy.rtc_config.debug bool\n",
      "                        Debug settings (default: False)\n",
      "  --policy.rtc_config.debug_maxlen int\n",
      "\n",
      "PI0FastConfig ['policy']:\n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        Device to use for the model (None = auto-detect)\n",
      "                        (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.paligemma_variant str\n",
      "  --policy.action_expert_variant str\n",
      "  --policy.dtype str    Options: \"bfloat16\", \"float32\" (default: float32)\n",
      "  --policy.chunk_size int\n",
      "                        Number of action steps to predict, in openpi called\n",
      "                        \"action_horizon\" (default: 50)\n",
      "  --policy.n_action_steps int\n",
      "                        Number of action steps to execute (default: 50)\n",
      "  --policy.max_state_dim int\n",
      "                        Shorter state and action vectors will be padded to\n",
      "                        these dimensions (default: 32)\n",
      "  --policy.max_action_dim int\n",
      "  --policy.max_action_tokens int\n",
      "  --policy.image_resolution int int\n",
      "  --policy.empty_cameras int\n",
      "                        see openpi `preprocessing_pytorch.py` Add empty\n",
      "                        images. Used to add empty cameras when no image\n",
      "                        features are present. (default: 0)\n",
      "  --policy.tokenizer_max_length int\n",
      "                        see openpi `__post_init__` (default: 200)\n",
      "  --policy.text_tokenizer_name str\n",
      "  --policy.action_tokenizer_name str\n",
      "  --policy.temperature float\n",
      "  --policy.max_decoding_steps int\n",
      "  --policy.fast_skip_tokens int\n",
      "  --policy.validate_action_token_prefix bool\n",
      "                        Whether to validate that decoded action tokens start\n",
      "                        with \"Action: \" prefix (default: True)\n",
      "  --policy.use_kv_cache bool\n",
      "                        Whether to use KV cache for faster autoregressive\n",
      "                        decoding (default: True)\n",
      "  --policy.normalization_mapping Dict\n",
      "  --policy.gradient_checkpointing bool\n",
      "                        Enable gradient checkpointing for memory optimization\n",
      "                        (default: False)\n",
      "  --policy.compile_model bool\n",
      "                        Whether to use torch.compile for model optimization\n",
      "                        (default: False)\n",
      "  --policy.compile_mode str\n",
      "                        Torch compile mode (default: max-autotune)\n",
      "  --policy.optimizer_lr float\n",
      "                        see openpi `CosineDecaySchedule: peak_lr` (default:\n",
      "                        2.5e-05)\n",
      "  --policy.optimizer_betas float float\n",
      "  --policy.optimizer_eps float\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.optimizer_grad_clip_norm float\n",
      "  --policy.scheduler_warmup_steps int\n",
      "                        Scheduler settings: see openpi `CosineDecaySchedule`\n",
      "                        Note: These will auto-scale if --steps <\n",
      "                        scheduler_decay_steps For example, --steps=3000 will\n",
      "                        scale warmup to 100 and decay to 3000 (default: 1000)\n",
      "  --policy.scheduler_decay_steps int\n",
      "  --policy.scheduler_decay_lr float\n",
      "\n",
      "Optional ['policy.rtc_config']:\n",
      "  Real-Time Chunking (RTC) configuration\n",
      "\n",
      "RTCConfig ['policy.rtc_config']:\n",
      "  Real-Time Chunking (RTC) configuration\n",
      "\n",
      "  --policy.rtc_config.enabled bool\n",
      "                        Infrastructure (default: False)\n",
      "  --policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule\n",
      "                        Core RTC settings Todo change to exp (default:\n",
      "                        RTCAttentionSchedule.LINEAR)\n",
      "  --policy.rtc_config.max_guidance_weight float\n",
      "  --policy.rtc_config.execution_horizon int\n",
      "  --policy.rtc_config.debug bool\n",
      "                        Debug settings (default: False)\n",
      "  --policy.rtc_config.debug_maxlen int\n",
      "\n",
      "PI05Config ['policy']:\n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        Device to use for the model (None = auto-detect)\n",
      "                        (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.paligemma_variant str\n",
      "  --policy.action_expert_variant str\n",
      "  --policy.dtype str    Options: \"bfloat16\", \"float32\" (default: float32)\n",
      "  --policy.chunk_size int\n",
      "                        Number of action steps to predict, in openpi called\n",
      "                        \"action_horizon\" (default: 50)\n",
      "  --policy.n_action_steps int\n",
      "                        Number of action steps to execute (default: 50)\n",
      "  --policy.max_state_dim int\n",
      "                        Shorter state and action vectors will be padded to\n",
      "                        these dimensions (default: 32)\n",
      "  --policy.max_action_dim int\n",
      "  --policy.num_inference_steps int\n",
      "                        Flow matching parameters: see openpi `PI0Pytorch`\n",
      "                        (default: 10)\n",
      "  --policy.time_sampling_beta_alpha float\n",
      "  --policy.time_sampling_beta_beta float\n",
      "  --policy.time_sampling_scale float\n",
      "  --policy.time_sampling_offset float\n",
      "  --policy.min_period float\n",
      "  --policy.max_period float\n",
      "  --policy.image_resolution int int\n",
      "  --policy.empty_cameras int\n",
      "                        see openpi `preprocessing_pytorch.py` Add empty\n",
      "                        images. Used to add empty cameras when no image\n",
      "                        features are present. (default: 0)\n",
      "  --policy.tokenizer_max_length int\n",
      "                        see openpi `__post_init__` (default: 200)\n",
      "  --policy.normalization_mapping Dict\n",
      "  --policy.gradient_checkpointing bool\n",
      "                        Enable gradient checkpointing for memory optimization\n",
      "                        (default: False)\n",
      "  --policy.compile_model bool\n",
      "                        Whether to use torch.compile for model optimization\n",
      "                        (default: False)\n",
      "  --policy.compile_mode str\n",
      "                        Torch compile mode (default: max-autotune)\n",
      "  --policy.freeze_vision_encoder bool\n",
      "                        Freeze only the vision encoder (default: False)\n",
      "  --policy.train_expert_only bool\n",
      "                        Freeze entire VLM, train only action expert and\n",
      "                        projections (default: False)\n",
      "  --policy.optimizer_lr float\n",
      "                        see openpi `CosineDecaySchedule: peak_lr` (default:\n",
      "                        2.5e-05)\n",
      "  --policy.optimizer_betas float float\n",
      "  --policy.optimizer_eps float\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.optimizer_grad_clip_norm float\n",
      "  --policy.scheduler_warmup_steps int\n",
      "                        Scheduler settings: see openpi `CosineDecaySchedule`\n",
      "                        Note: These will auto-scale if --steps <\n",
      "                        scheduler_decay_steps For example, --steps=3000 will\n",
      "                        scale warmup to 100 and decay to 3000 (default: 1000)\n",
      "  --policy.scheduler_decay_steps int\n",
      "  --policy.scheduler_decay_lr float\n",
      "\n",
      "Optional ['policy.rtc_config']:\n",
      "  Real-Time Chunking (RTC) configuration\n",
      "\n",
      "RTCConfig ['policy.rtc_config']:\n",
      "  Real-Time Chunking (RTC) configuration\n",
      "\n",
      "  --policy.rtc_config.enabled bool\n",
      "                        Infrastructure (default: False)\n",
      "  --policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule\n",
      "                        Core RTC settings Todo change to exp (default:\n",
      "                        RTCAttentionSchedule.LINEAR)\n",
      "  --policy.rtc_config.max_guidance_weight float\n",
      "  --policy.rtc_config.execution_horizon int\n",
      "  --policy.rtc_config.debug bool\n",
      "                        Debug settings (default: False)\n",
      "  --policy.rtc_config.debug_maxlen int\n",
      "\n",
      "SmolVLAConfig ['policy']:\n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "                        Input / output structure. (default: 1)\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.chunk_size int\n",
      "  --policy.n_action_steps int\n",
      "  --policy.normalization_mapping Dict\n",
      "  --policy.max_state_dim int\n",
      "                        Shorter state and action vectors will be padded\n",
      "                        (default: 32)\n",
      "  --policy.max_action_dim int\n",
      "  --policy.resize_imgs_with_padding int int\n",
      "                        Image preprocessing (default: (512, 512))\n",
      "  --policy.empty_cameras int\n",
      "                        Add empty images. Used by smolvla_aloha_sim which adds\n",
      "                        the empty left and right wrist cameras in addition to\n",
      "                        the top camera. (default: 0)\n",
      "  --policy.adapt_to_pi_aloha bool\n",
      "                        Converts the joint and gripper values from the\n",
      "                        standard Aloha space to the space used by the pi\n",
      "                        internal runtime which was used to train the base\n",
      "                        model. (default: False)\n",
      "  --policy.use_delta_joint_actions_aloha bool\n",
      "                        Converts joint dimensions to deltas with respect to\n",
      "                        the current state before passing to the model. Gripper\n",
      "                        dimensions will remain in absolute values. (default:\n",
      "                        False)\n",
      "  --policy.tokenizer_max_length int\n",
      "                        Tokenizer (default: 48)\n",
      "  --policy.num_steps int\n",
      "                        Decoding (default: 10)\n",
      "  --policy.use_cache bool\n",
      "                        Attention utils (default: True)\n",
      "  --policy.freeze_vision_encoder bool\n",
      "                        Finetuning settings (default: True)\n",
      "  --policy.train_expert_only bool\n",
      "  --policy.train_state_proj bool\n",
      "  --policy.optimizer_lr float\n",
      "                        Training presets (default: 0.0001)\n",
      "  --policy.optimizer_betas float float\n",
      "  --policy.optimizer_eps float\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.optimizer_grad_clip_norm float\n",
      "  --policy.scheduler_warmup_steps int\n",
      "  --policy.scheduler_decay_steps int\n",
      "  --policy.scheduler_decay_lr float\n",
      "  --policy.vlm_model_name str\n",
      "                        Select the VLM backbone. (default:\n",
      "                        HuggingFaceTB/SmolVLM2-500M-Video-Instruct)\n",
      "  --policy.load_vlm_weights bool\n",
      "                        Set to True in case of training the expert from\n",
      "                        scratch. True when init from pretrained SmolVLA\n",
      "                        weights (default: False)\n",
      "  --policy.add_image_special_tokens bool\n",
      "                        Whether to use special image tokens around image\n",
      "                        features. (default: False)\n",
      "  --policy.attention_mode str\n",
      "  --policy.prefix_length int\n",
      "  --policy.pad_language_to str\n",
      "                        \"max_length\" (default: longest)\n",
      "  --policy.num_expert_layers int\n",
      "                        Less or equal to 0 is the default where the action\n",
      "                        expert has the same number of layers of VLM. Otherwise\n",
      "                        the expert have less layers. (default: -1)\n",
      "  --policy.num_vlm_layers int\n",
      "                        Number of layers used in the VLM (first num_vlm_layers\n",
      "                        layers) (default: 16)\n",
      "  --policy.self_attn_every_n_layers int\n",
      "                        Interleave SA layers each self_attn_every_n_layers\n",
      "                        (default: 2)\n",
      "  --policy.expert_width_multiplier float\n",
      "                        The action expert hidden size (wrt to the VLM)\n",
      "                        (default: 0.75)\n",
      "  --policy.min_period float\n",
      "                        sensitivity range for the timestep used in sine-cosine\n",
      "                        positional encoding (default: 0.004)\n",
      "  --policy.max_period float\n",
      "\n",
      "Optional ['policy.rtc_config']:\n",
      "  Real-Time Chunking (RTC) configuration\n",
      "\n",
      "RTCConfig ['policy.rtc_config']:\n",
      "  Real-Time Chunking (RTC) configuration\n",
      "\n",
      "  --policy.rtc_config.enabled bool\n",
      "                        Infrastructure (default: False)\n",
      "  --policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule\n",
      "                        Core RTC settings Todo change to exp (default:\n",
      "                        RTCAttentionSchedule.LINEAR)\n",
      "  --policy.rtc_config.max_guidance_weight float\n",
      "  --policy.rtc_config.execution_horizon int\n",
      "  --policy.rtc_config.debug bool\n",
      "                        Debug settings (default: False)\n",
      "  --policy.rtc_config.debug_maxlen int\n",
      "\n",
      "TDMPCConfig ['policy']:\n",
      "  Configuration class for TDMPCPolicy.\n",
      "  \n",
      "      Defaults are configured for training with xarm_lift_medium_replay providing proprioceptive and single\n",
      "      camera observations.\n",
      "  \n",
      "      The parameters you will most likely need to change are the ones which depend on the environment / sensors.\n",
      "      Those are: `input_shapes`, `output_shapes`, and perhaps `max_random_shift_ratio`.\n",
      "  \n",
      "      Args:\n",
      "          n_action_repeats: The number of times to repeat the action returned by the planning. (hint: Google\n",
      "              action repeats in Q-learning or ask your favorite chatbot)\n",
      "          horizon: Horizon for model predictive control.\n",
      "          n_action_steps: Number of action steps to take from the plan given by model predictive control. This\n",
      "              is an alternative to using action repeats. If this is set to more than 1, then we require\n",
      "              `n_action_repeats == 1`, `use_mpc == True` and `n_action_steps <= horizon`. Note that this\n",
      "              approach of using multiple steps from the plan is not in the original implementation.\n",
      "          input_shapes: A dictionary defining the shapes of the input data for the policy. The key represents\n",
      "              the input data name, and the value is a list indicating the dimensions of the corresponding data.\n",
      "              For example, \"observation.image\" refers to an input from a camera with dimensions [3, 96, 96],\n",
      "              indicating it has three color channels and 96x96 resolution. Importantly, `input_shapes` doesn't\n",
      "              include batch dimension or temporal dimension.\n",
      "          output_shapes: A dictionary defining the shapes of the output data for the policy. The key represents\n",
      "              the output data name, and the value is a list indicating the dimensions of the corresponding data.\n",
      "              For example, \"action\" refers to an output shape of [14], indicating 14-dimensional actions.\n",
      "              Importantly, `output_shapes` doesn't include batch dimension or temporal dimension.\n",
      "          input_normalization_modes: A dictionary with key representing the modality (e.g. \"observation.state\"),\n",
      "              and the value specifies the normalization mode to apply. The two available modes are \"mean_std\"\n",
      "              which subtracts the mean and divides by the standard deviation and \"min_max\" which rescale in a\n",
      "              [-1, 1] range. Note that here this defaults to None meaning inputs are not normalized. This is to\n",
      "              match the original implementation.\n",
      "          output_normalization_modes: Similar dictionary as `normalize_input_modes`, but to unnormalize to the\n",
      "              original scale. Note that this is also used for normalizing the training targets. NOTE: Clipping\n",
      "              to [-1, +1] is used during MPPI/CEM. Therefore, it is recommended that you stick with \"min_max\"\n",
      "              normalization mode here.\n",
      "          image_encoder_hidden_dim: Number of channels for the convolutional layers used for image encoding.\n",
      "          state_encoder_hidden_dim: Hidden dimension for MLP used for state vector encoding.\n",
      "          latent_dim: Observation's latent embedding dimension.\n",
      "          q_ensemble_size: Number of Q function estimators to use in an ensemble for uncertainty estimation.\n",
      "          mlp_dim: Hidden dimension of MLPs used for modelling the dynamics encoder, reward function, policy\n",
      "              (Ï€), Q ensemble, and V.\n",
      "          discount: Discount factor (Î³) to use for the reinforcement learning formalism.\n",
      "          use_mpc: Whether to use model predictive control. The alternative is to just sample the policy model\n",
      "              (Ï€) for each step.\n",
      "          cem_iterations: Number of iterations for the MPPI/CEM loop in MPC.\n",
      "          max_std: Maximum standard deviation for actions sampled from the gaussian PDF in CEM.\n",
      "          min_std: Minimum standard deviation for noise applied to actions sampled from the policy model (Ï€).\n",
      "              Doubles up as the minimum standard deviation for actions sampled from the gaussian PDF in CEM.\n",
      "          n_gaussian_samples: Number of samples to draw from the gaussian distribution every CEM iteration. Must\n",
      "              be non-zero.\n",
      "          n_pi_samples: Number of samples to draw from the policy / world model rollout every CEM iteration. Can\n",
      "              be zero.\n",
      "          uncertainty_regularizer_coeff: Coefficient for the uncertainty regularization used when estimating\n",
      "              trajectory values (this is the Î» coefficient in eqn 4 of FOWM).\n",
      "          n_elites: The number of elite samples to use for updating the gaussian parameters every CEM iteration.\n",
      "          elite_weighting_temperature: The temperature to use for softmax weighting (by trajectory value) of the\n",
      "              elites, when updating the gaussian parameters for CEM.\n",
      "          gaussian_mean_momentum: Momentum (Î±) used for EMA updates of the mean parameter Î¼ of the gaussian\n",
      "              parameters optimized in CEM. Updates are calculated as Î¼â» â† Î±Î¼â» + (1-Î±)Î¼.\n",
      "          max_random_shift_ratio: Maximum random shift (as a proportion of the image size) to apply to the\n",
      "              image(s) (in units of pixels) for training-time augmentation. If set to 0, no such augmentation\n",
      "              is applied. Note that the input images are assumed to be square for this augmentation.\n",
      "          reward_coeff: Loss weighting coefficient for the reward regression loss.\n",
      "          expectile_weight: Weighting (Ï„) used in expectile regression for the state value function (V).\n",
      "              v_pred < v_target is weighted by Ï„ and v_pred >= v_target is weighted by (1-Ï„). Ï„ is expected to\n",
      "              be in [0, 1]. Setting Ï„ closer to 1 results in a more \"optimistic\" V. This is sensible to do\n",
      "              because v_target is obtained by evaluating the learned state-action value functions (Q) with\n",
      "              in-sample actions that may not be always optimal.\n",
      "          value_coeff: Loss weighting coefficient for both the state-action value (Q) TD loss, and the state\n",
      "              value (V) expectile regression loss.\n",
      "          consistency_coeff: Loss weighting coefficient for the consistency loss.\n",
      "          advantage_scaling: A factor by which the advantages are scaled prior to exponentiation for advantage\n",
      "              weighted regression of the policy (Ï€) estimator parameters. Note that the exponentiated advantages\n",
      "              are clamped at 100.0.\n",
      "          pi_coeff: Loss weighting coefficient for the action regression loss.\n",
      "          temporal_decay_coeff: Exponential decay coefficient for decaying the loss coefficient for future time-\n",
      "              steps. Hint: each loss computation involves `horizon` steps worth of actions starting from the\n",
      "              current time step.\n",
      "          target_model_momentum: Momentum (Î±) used for EMA updates of the target models. Updates are calculated\n",
      "              as Ï• â† Î±Ï• + (1-Î±)Î¸ where Ï• are the parameters of the target model and Î¸ are the parameters of the\n",
      "              model being trained.\n",
      "      \n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "                        Input / output structure. (default: 1)\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.n_action_repeats int\n",
      "  --policy.horizon int  \n",
      "  --policy.n_action_steps int\n",
      "  --policy.normalization_mapping Dict\n",
      "  --policy.image_encoder_hidden_dim int\n",
      "  --policy.state_encoder_hidden_dim int\n",
      "  --policy.latent_dim int\n",
      "  --policy.q_ensemble_size int\n",
      "  --policy.mlp_dim int  \n",
      "  --policy.discount float\n",
      "  --policy.use_mpc bool\n",
      "  --policy.cem_iterations int\n",
      "  --policy.max_std float\n",
      "  --policy.min_std float\n",
      "  --policy.n_gaussian_samples int\n",
      "  --policy.n_pi_samples int\n",
      "  --policy.uncertainty_regularizer_coeff float\n",
      "  --policy.n_elites int\n",
      "  --policy.elite_weighting_temperature float\n",
      "  --policy.gaussian_mean_momentum float\n",
      "  --policy.max_random_shift_ratio float\n",
      "  --policy.reward_coeff float\n",
      "  --policy.expectile_weight float\n",
      "  --policy.value_coeff float\n",
      "  --policy.consistency_coeff float\n",
      "  --policy.advantage_scaling float\n",
      "  --policy.pi_coeff float\n",
      "  --policy.temporal_decay_coeff float\n",
      "  --policy.target_model_momentum float\n",
      "  --policy.optimizer_lr float\n",
      "                        Training presets (default: 0.0003)\n",
      "\n",
      "VQBeTConfig ['policy']:\n",
      "  Configuration class for VQ-BeT.\n",
      "  \n",
      "      Defaults are configured for training with PushT providing proprioceptive and single camera observations.\n",
      "  \n",
      "      The parameters you will most likely need to change are the ones which depend on the environment / sensors.\n",
      "      Those are: `input_shapes` and `output_shapes`.\n",
      "  \n",
      "      Notes on the inputs and outputs:\n",
      "          - \"observation.state\" is required as an input key.\n",
      "          - At least one key starting with \"observation.image is required as an input.\n",
      "          - If there are multiple keys beginning with \"observation.image\" they are treated as multiple camera\n",
      "            views. Right now we only support all images having the same shape.\n",
      "          - \"action\" is required as an output key.\n",
      "  \n",
      "      Args:\n",
      "          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the\n",
      "              current step and additional steps going back).\n",
      "          n_action_pred_token: Total number of current token and future tokens that VQ-BeT predicts.\n",
      "          action_chunk_size: Action chunk size of each action prediction token.\n",
      "          input_shapes: A dictionary defining the shapes of the input data for the policy.\n",
      "              The key represents the input data name, and the value is a list indicating the dimensions\n",
      "              of the corresponding data. For example, \"observation.image\" refers to an input from\n",
      "              a camera with dimensions [3, 96, 96], indicating it has three color channels and 96x96 resolution.\n",
      "              Importantly, shapes doesnt include batch dimension or temporal dimension.\n",
      "          output_shapes: A dictionary defining the shapes of the output data for the policy.\n",
      "              The key represents the output data name, and the value is a list indicating the dimensions\n",
      "              of the corresponding data. For example, \"action\" refers to an output shape of [14], indicating\n",
      "              14-dimensional actions. Importantly, shapes doesnt include batch dimension or temporal dimension.\n",
      "          input_normalization_modes: A dictionary with key representing the modality (e.g. \"observation.state\"),\n",
      "              and the value specifies the normalization mode to apply. The two available modes are \"mean_std\"\n",
      "              which subtracts the mean and divides by the standard deviation and \"min_max\" which rescale in a\n",
      "              [-1, 1] range.\n",
      "          output_normalization_modes: Similar dictionary as `normalize_input_modes`, but to unnormalize to the\n",
      "              original scale. Note that this is also used for normalizing the training targets.\n",
      "          vision_backbone: Name of the torchvision resnet backbone to use for encoding images.\n",
      "          crop_shape: (H, W) shape to crop images to as a preprocessing step for the vision backbone. Must fit\n",
      "              within the image size. If None, no cropping is done.\n",
      "          crop_is_random: Whether the crop should be random at training time (it's always a center crop in eval\n",
      "              mode).\n",
      "          pretrained_backbone_weights: Pretrained weights from torchvision to initialize the backbone.\n",
      "              `None` means no pretrained weights.\n",
      "          use_group_norm: Whether to replace batch normalization with group normalization in the backbone.\n",
      "              The group sizes are set to be about 16 (to be precise, feature_dim // 16).\n",
      "          spatial_softmax_num_keypoints: Number of keypoints for SpatialSoftmax.\n",
      "          n_vqvae_training_steps: Number of optimization steps for training Residual VQ.\n",
      "          vqvae_n_embed: Number of embedding vectors in the RVQ dictionary (each layer).\n",
      "          vqvae_embedding_dim: Dimension of each embedding vector in the RVQ dictionary.\n",
      "          vqvae_enc_hidden_dim: Size of hidden dimensions of Encoder / Decoder part of Residaul VQ-VAE\n",
      "          gpt_block_size: Max block size of minGPT (should be larger than the number of input tokens)\n",
      "          gpt_input_dim: Size of output input of GPT. This is also used as the dimension of observation features.\n",
      "          gpt_output_dim: Size of output dimension of GPT. This is also used as a input dimension of offset / bin prediction headers.\n",
      "          gpt_n_layer: Number of layers of GPT\n",
      "          gpt_n_head: Number of headers of GPT\n",
      "          gpt_hidden_dim: Size of hidden dimensions of GPT\n",
      "          dropout: Dropout rate for GPT\n",
      "          offset_loss_weight:  A constant that is multiplied to the offset loss\n",
      "          primary_code_loss_weight: A constant that is multiplied to the primary code prediction loss\n",
      "          secondary_code_loss_weight: A constant that is multiplied to the secondary code prediction loss\n",
      "          bet_softmax_temperature: Sampling temperature of code for rollout with VQ-BeT\n",
      "          sequentially_select: Whether select code of primary / secondary as sequentially (pick primary code,\n",
      "              and then select secodnary code), or at the same time.\n",
      "      \n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.n_action_pred_token int\n",
      "  --policy.action_chunk_size int\n",
      "  --policy.normalization_mapping Dict\n",
      "  --policy.vision_backbone str\n",
      "  --policy.crop_shape [int int]\n",
      "  --policy.crop_is_random bool\n",
      "  --policy.pretrained_backbone_weights [str]\n",
      "  --policy.use_group_norm bool\n",
      "  --policy.spatial_softmax_num_keypoints int\n",
      "  --policy.n_vqvae_training_steps int\n",
      "  --policy.vqvae_n_embed int\n",
      "  --policy.vqvae_embedding_dim int\n",
      "  --policy.vqvae_enc_hidden_dim int\n",
      "  --policy.gpt_block_size int\n",
      "  --policy.gpt_input_dim int\n",
      "  --policy.gpt_output_dim int\n",
      "  --policy.gpt_n_layer int\n",
      "  --policy.gpt_n_head int\n",
      "  --policy.gpt_hidden_dim int\n",
      "  --policy.dropout float\n",
      "  --policy.offset_loss_weight float\n",
      "  --policy.primary_code_loss_weight float\n",
      "  --policy.secondary_code_loss_weight float\n",
      "  --policy.bet_softmax_temperature float\n",
      "  --policy.sequentially_select bool\n",
      "  --policy.optimizer_lr float\n",
      "                        Training presets (default: 0.0001)\n",
      "  --policy.optimizer_betas Any\n",
      "  --policy.optimizer_eps float\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.optimizer_vqvae_lr float\n",
      "  --policy.optimizer_vqvae_weight_decay float\n",
      "  --policy.scheduler_warmup_steps int\n",
      "\n",
      "WallXConfig ['policy']:\n",
      "  \n",
      "      Configuration class for Wall-X policy.\n",
      "  \n",
      "      Wall-X is based on Qwen2.5-VL with action prediction capabilities using flow matching.\n",
      "      It supports cross-embodiment robotic control through unified action representations.\n",
      "  \n",
      "      This config supports multi-modal learning with vision, language, and action data.\n",
      "      \n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "                        ==================== Input / Output Structure\n",
      "                        ==================== (default: 1)\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.chunk_size int\n",
      "                        action_horizon in wall-x (default: 32)\n",
      "  --policy.n_action_steps int\n",
      "  --policy.max_action_dim int\n",
      "                        Action dimension - wall-x uses 20 (default: 20)\n",
      "  --policy.max_state_dim int\n",
      "                        For proprioception (default: 20)\n",
      "  --policy.normalization_mapping Dict\n",
      "  --policy.pretrained_name_or_path str\n",
      "                        ==================== Action Prediction\n",
      "                        ==================== Pretrained model paths (default:\n",
      "                        x-square-robot/wall-oss-flow)\n",
      "  --policy.action_tokenizer_path [str]\n",
      "                        Tokenizer settings (default: physical-\n",
      "                        intelligence/fast)\n",
      "  --policy.prediction_mode str\n",
      "                        Action prediction mode: \"diffusion\" or \"fast\"\n",
      "                        (default: diffusion)\n",
      "  --policy.attn_implementation str\n",
      "                        Attention Implementation, options: \"eager\",\n",
      "                        \"flash_attention_2\", \"sdpa\" NOTE: flash-\n",
      "                        attn==2.7.4.post1 is required for flash_attention_2\n",
      "                        implementation (default: eager)\n",
      "  --policy.optimizer_lr float\n",
      "                        ==================== Optimizer Presets\n",
      "                        ==================== (default: 2e-05)\n",
      "  --policy.optimizer_betas float float\n",
      "  --policy.optimizer_eps float\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.optimizer_grad_clip_norm float\n",
      "  --policy.scheduler_warmup_steps int\n",
      "  --policy.scheduler_decay_steps int\n",
      "  --policy.scheduler_decay_lr float\n",
      "\n",
      "XVLAConfig ['policy']:\n",
      "  \n",
      "      Configuration class for the XVLA (Extended Vision-Language-Action) policy so it can\n",
      "      plug into the LeRobot training stack.\n",
      "  \n",
      "      The config mirrors the knobs exposed in the original XVLA repository but also\n",
      "      declares the input/output feature contract required by LeRobot.\n",
      "      \n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "                        Input / output structure (default: 1)\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device [str]\n",
      "                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.chunk_size int\n",
      "  --policy.n_action_steps int\n",
      "  --policy.dtype str    Options: \"bfloat16\", \"float32\" (default: float32)\n",
      "  --policy.normalization_mapping Dict\n",
      "  --policy.florence_config Dict\n",
      "                        Florence2 backbone and tokenizer configuration\n",
      "                        (default: {})\n",
      "  --policy.tokenizer_name str\n",
      "  --policy.tokenizer_max_length int\n",
      "  --policy.tokenizer_padding_side str\n",
      "  --policy.pad_language_to str\n",
      "  --policy.hidden_size int\n",
      "                        Transformer head (default: 1024)\n",
      "  --policy.depth int    \n",
      "  --policy.num_heads int\n",
      "  --policy.mlp_ratio float\n",
      "  --policy.num_domains int\n",
      "  --policy.len_soft_prompts int\n",
      "  --policy.dim_time int\n",
      "  --policy.max_len_seq int\n",
      "  --policy.use_hetero_proj bool\n",
      "  --policy.action_mode str\n",
      "                        Action & proprioception (default: ee6d)\n",
      "  --policy.num_denoising_steps int\n",
      "  --policy.use_proprio bool\n",
      "  --policy.max_state_dim int\n",
      "  --policy.max_action_dim int\n",
      "                        Maximum action dimension for padding (used by \"auto\"\n",
      "                        action mode) (default: 20)\n",
      "  --policy.domain_feature_key [str]\n",
      "  --policy.resize_imgs_with_padding [int int]\n",
      "                        Vision preprocessing (default: None)\n",
      "  --policy.num_image_views [int]\n",
      "  --policy.empty_cameras int\n",
      "  --policy.freeze_vision_encoder bool\n",
      "                        Freeze VLM vision encoder weights (default: False)\n",
      "  --policy.freeze_language_encoder bool\n",
      "                        Freeze VLM language encoder weights (default: False)\n",
      "  --policy.train_policy_transformer bool\n",
      "                        Allow policy transformer to train (default: True)\n",
      "  --policy.train_soft_prompts bool\n",
      "                        Allow soft prompts to train (default: True)\n",
      "  --policy.optimizer_lr float\n",
      "                        Training presets (default: 0.0001)\n",
      "  --policy.optimizer_betas float float\n",
      "  --policy.optimizer_eps float\n",
      "  --policy.optimizer_weight_decay float\n",
      "  --policy.optimizer_grad_clip_norm float\n",
      "  --policy.optimizer_soft_prompt_lr_scale float\n",
      "                        Scale factor for soft-prompt LR (default: 1.0)\n",
      "  --policy.optimizer_soft_prompt_warmup_lr_scale [float]\n",
      "                        Start scale for warmup (e.g., 0.01) (default: None)\n",
      "  --policy.scheduler_warmup_steps int\n",
      "  --policy.scheduler_decay_steps int\n",
      "  --policy.scheduler_decay_lr float\n",
      "\n",
      "SACConfig ['policy']:\n",
      "  Soft Actor-Critic (SAC) configuration.\n",
      "  \n",
      "      SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy\n",
      "      reinforcement learning framework. It learns a policy and a Q-function simultaneously\n",
      "      using experience collected from the environment.\n",
      "  \n",
      "      This configuration class contains all the parameters needed to define a SAC agent,\n",
      "      including network architectures, optimization settings, and algorithm-specific\n",
      "      hyperparameters.\n",
      "      \n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device str   Architecture specifics Device to run the model on\n",
      "                        (e.g., \"cuda\", \"cpu\") (default: cpu)\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.normalization_mapping Dict\n",
      "                        Mapping of feature types to normalization modes\n",
      "                        (default: {'VISUAL': <NormalizationMode.MEAN_STD:\n",
      "                        'MEAN_STD'>, 'STATE': <NormalizationMode.MIN_MAX:\n",
      "                        'MIN_MAX'>, 'ENV': <NormalizationMode.MIN_MAX:\n",
      "                        'MIN_MAX'>, 'ACTION': <NormalizationMode.MIN_MAX:\n",
      "                        'MIN_MAX'>})\n",
      "  --policy.dataset_stats [Dict]\n",
      "                        Statistics for normalizing different types of inputs\n",
      "                        (default: {'observation.image': {'mean': [0.485,\n",
      "                        0.456, 0.406], 'std': [0.229, 0.224, 0.225]},\n",
      "                        'observation.state': {'min': [0.0, 0.0], 'max': [1.0,\n",
      "                        1.0]}, 'action': {'min': [0.0, 0.0, 0.0], 'max': [1.0,\n",
      "                        1.0, 1.0]}})\n",
      "  --policy.storage_device str\n",
      "                        Device to store the model on (default: cpu)\n",
      "  --policy.vision_encoder_name [str]\n",
      "                        Name of the vision encoder model (Set to\n",
      "                        \"helper2424/resnet10\" for hil serl resnet10) (default:\n",
      "                        None)\n",
      "  --policy.freeze_vision_encoder bool\n",
      "                        Whether to freeze the vision encoder during training\n",
      "                        (default: True)\n",
      "  --policy.image_encoder_hidden_dim int\n",
      "                        Hidden dimension size for the image encoder (default:\n",
      "                        32)\n",
      "  --policy.shared_encoder bool\n",
      "                        Whether to use a shared encoder for actor and critic\n",
      "                        (default: True)\n",
      "  --policy.num_discrete_actions [int]\n",
      "                        Number of discrete actions, eg for gripper actions\n",
      "                        (default: None)\n",
      "  --policy.image_embedding_pooling_dim int\n",
      "                        Dimension of the image embedding pooling (default: 8)\n",
      "  --policy.online_steps int\n",
      "                        Training parameter Number of steps for online training\n",
      "                        (default: 1000000)\n",
      "  --policy.online_buffer_capacity int\n",
      "                        Capacity of the online replay buffer (default: 100000)\n",
      "  --policy.offline_buffer_capacity int\n",
      "                        Capacity of the offline replay buffer (default:\n",
      "                        100000)\n",
      "  --policy.async_prefetch bool\n",
      "                        Whether to use asynchronous prefetching for the\n",
      "                        buffers (default: False)\n",
      "  --policy.online_step_before_learning int\n",
      "                        Number of steps before learning starts (default: 100)\n",
      "  --policy.policy_update_freq int\n",
      "                        Frequency of policy updates (default: 1)\n",
      "  --policy.discount float\n",
      "                        SAC algorithm parameters Discount factor for the SAC\n",
      "                        algorithm (default: 0.99)\n",
      "  --policy.temperature_init float\n",
      "                        Initial temperature value (default: 1.0)\n",
      "  --policy.num_critics int\n",
      "                        Number of critics in the ensemble (default: 2)\n",
      "  --policy.num_subsample_critics [int]\n",
      "                        Number of subsampled critics for training (default:\n",
      "                        None)\n",
      "  --policy.critic_lr float\n",
      "                        Learning rate for the critic network (default: 0.0003)\n",
      "  --policy.actor_lr float\n",
      "                        Learning rate for the actor network (default: 0.0003)\n",
      "  --policy.temperature_lr float\n",
      "                        Learning rate for the temperature parameter (default:\n",
      "                        0.0003)\n",
      "  --policy.critic_target_update_weight float\n",
      "                        Weight for the critic target update (default: 0.005)\n",
      "  --policy.utd_ratio int\n",
      "                        Update-to-data ratio for the UTD algorithm (If you\n",
      "                        want enable utd_ratio, you need to set it to >1)\n",
      "                        (default: 1)\n",
      "  --policy.state_encoder_hidden_dim int\n",
      "                        Hidden dimension size for the state encoder (default:\n",
      "                        256)\n",
      "  --policy.latent_dim int\n",
      "                        Dimension of the latent space (default: 256)\n",
      "  --policy.target_entropy [float]\n",
      "                        Target entropy for the SAC algorithm (default: None)\n",
      "  --policy.use_backup_entropy bool\n",
      "                        Whether to use backup entropy for the SAC algorithm\n",
      "                        (default: True)\n",
      "  --policy.grad_clip_norm float\n",
      "                        Gradient clipping norm for the SAC algorithm (default:\n",
      "                        40.0)\n",
      "  --policy.use_torch_compile bool\n",
      "                        Optimizations (default: True)\n",
      "\n",
      "CriticNetworkConfig ['policy.critic_network_kwargs']:\n",
      "  Network configuration\n",
      "  Configuration for the critic network architecture\n",
      "\n",
      "  --policy.critic_network_kwargs.hidden_dims List\n",
      "  --policy.critic_network_kwargs.activate_final bool\n",
      "  --policy.critic_network_kwargs.final_activation [str]\n",
      "\n",
      "ActorNetworkConfig ['policy.actor_network_kwargs']:\n",
      "  Configuration for the actor network architecture\n",
      "\n",
      "  --policy.actor_network_kwargs.hidden_dims List\n",
      "  --policy.actor_network_kwargs.activate_final bool\n",
      "\n",
      "PolicyConfig ['policy.policy_kwargs']:\n",
      "  Configuration for the policy parameters\n",
      "\n",
      "  --policy.policy_kwargs.use_tanh_squash bool\n",
      "  --policy.policy_kwargs.std_min float\n",
      "  --policy.policy_kwargs.std_max float\n",
      "  --policy.policy_kwargs.init_final float\n",
      "\n",
      "CriticNetworkConfig ['policy.discrete_critic_network_kwargs']:\n",
      "  Configuration for the discrete critic network\n",
      "\n",
      "  --policy.discrete_critic_network_kwargs.hidden_dims List\n",
      "  --policy.discrete_critic_network_kwargs.activate_final bool\n",
      "  --policy.discrete_critic_network_kwargs.final_activation [str]\n",
      "\n",
      "ActorLearnerConfig ['policy.actor_learner_config']:\n",
      "  Configuration for actor-learner architecture\n",
      "\n",
      "  --policy.actor_learner_config.learner_host str\n",
      "  --policy.actor_learner_config.learner_port int\n",
      "  --policy.actor_learner_config.policy_parameters_push_frequency int\n",
      "  --policy.actor_learner_config.queue_get_timeout float\n",
      "\n",
      "ConcurrencyConfig ['policy.concurrency']:\n",
      "  Configuration for concurrency settings (you can use threads or processes for the actor and learner)\n",
      "\n",
      "  --policy.concurrency.actor str\n",
      "  --policy.concurrency.learner str\n",
      "\n",
      "RewardClassifierConfig ['policy']:\n",
      "  Configuration for the Reward Classifier model.\n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "  --policy.input_features [Dict]\n",
      "                        `input_features` can be set to None/null in order to\n",
      "                        infer those values from the dataset. (default: {})\n",
      "  --policy.output_features [Dict]\n",
      "  --policy.device str   \n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.name str     \n",
      "  --policy.num_classes int\n",
      "  --policy.hidden_dim int\n",
      "  --policy.latent_dim int\n",
      "  --policy.image_embedding_pooling_dim int\n",
      "  --policy.dropout_rate float\n",
      "  --policy.model_name str\n",
      "  --policy.model_type str\n",
      "                        \"transformer\" or \"cnn\" (default: cnn)\n",
      "  --policy.num_cameras int\n",
      "  --policy.learning_rate float\n",
      "  --policy.weight_decay float\n",
      "  --policy.grad_clip_norm float\n",
      "  --policy.normalization_mapping Dict\n",
      "\n",
      "SARMConfig ['policy']:\n",
      "  Configuration class for SARM (Stage-Aware Reward Modeling).\n",
      "  \n",
      "      Supports three annotation modes:\n",
      "  \n",
      "      1. single_stage (default): No annotations needed. Uses the episode's task description\n",
      "         as a single stage covering the entire episode.\n",
      "  \n",
      "      2. dense_only: Uses dense (fine-grained) annotations from VLM, with an auto-generated\n",
      "         single sparse \"task\" stage covering the full episode. The dense head learns detailed\n",
      "         subtask progression while sparse provides overall task completion.\n",
      "  \n",
      "      3. dual: Full dual-head mode with both sparse (high-level) and dense (fine-grained)\n",
      "         annotations from VLM. Both heads are trained on their respective annotations.\n",
      "  \n",
      "      The annotation_mode determines how sparse_temporal_proportions and dense_temporal_proportions\n",
      "      are loaded/generated during model initialization.\n",
      "      \n",
      "\n",
      "  --policy.n_obs_steps int\n",
      "                        Number of observation history steps (default: 8)\n",
      "  --policy.input_features dict\n",
      "                        Populated by the processor (video_features,\n",
      "                        state_features, text_features) (default: {})\n",
      "  --policy.output_features dict\n",
      "                        Output features (updated in __post_init__) (default:\n",
      "                        {'stage': PolicyFeature(type=<FeatureType.REWARD:\n",
      "                        'REWARD'>, shape=(9, 5)), 'progress':\n",
      "                        PolicyFeature(type=<FeatureType.REWARD: 'REWARD'>,\n",
      "                        shape=(9, 1))})\n",
      "  --policy.device [str]\n",
      "  --policy.use_amp bool\n",
      "                        `use_amp` determines whether to use Automatic Mixed\n",
      "                        Precision (AMP) for training and evaluation. With AMP,\n",
      "                        automatic gradient scaling is used. (default: False)\n",
      "  --policy.use_peft bool\n",
      "                        Whether the policy employed PEFT for training.\n",
      "                        (default: False)\n",
      "  --policy.push_to_hub bool\n",
      "                        type: ignore[assignment] # TODO: use a different name\n",
      "                        to avoid override (default: True)\n",
      "  --policy.repo_id [str]\n",
      "  --policy.private [bool]\n",
      "                        Upload on private repository on the Hugging Face hub.\n",
      "                        (default: None)\n",
      "  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n",
      "  --policy.license [str]\n",
      "                        Add tags to your policy on the hub. (default: None)\n",
      "  --policy.pretrained_path [Path]\n",
      "                        Either the repo ID of a model hosted on the Hub or a\n",
      "                        path to a directory containing weights saved using\n",
      "                        `Policy.save_pretrained`. If not provided, the policy\n",
      "                        is initialized from scratch. (default: None)\n",
      "  --policy.annotation_mode str\n",
      "                        \"single_stage\", \"dense_only\", or \"dual\" (default:\n",
      "                        single_stage)\n",
      "  --policy.frame_gap int\n",
      "                        Frame gap between frames (at 30 fps = 1 second)\n",
      "                        (default: 30)\n",
      "  --policy.max_rewind_steps int\n",
      "                        Maximum rewind steps for temporal augmentation\n",
      "                        (default: 4)\n",
      "  --policy.image_dim int\n",
      "                        Total frames = 1 + n_obs_steps + max_rewind_steps\n",
      "                        (computed in property) During training with rewind:\n",
      "                        [obs_frames] + [rewind_frames] During inference:\n",
      "                        [obs_frames] only Architecture params (default: 512)\n",
      "  --policy.text_dim int\n",
      "  --policy.hidden_dim int\n",
      "  --policy.num_heads int\n",
      "  --policy.num_layers int\n",
      "  --policy.max_state_dim int\n",
      "  --policy.drop_n_last_frames int\n",
      "  --policy.batch_size int\n",
      "  --policy.clip_batch_size int\n",
      "  --policy.dropout float\n",
      "  --policy.stage_loss_weight float\n",
      "                        Weight for stage classification loss when using\n",
      "                        subtask annotations (default: 1.0)\n",
      "  --policy.rewind_probability float\n",
      "  --policy.language_perturbation_probability float\n",
      "  --policy.num_sparse_stages int\n",
      "                        Sparse annotations (high-level stages) (default: 1)\n",
      "  --policy.sparse_subtask_names [list]\n",
      "  --policy.sparse_temporal_proportions [list]\n",
      "  --policy.num_dense_stages [int]\n",
      "                        Dense annotations (fine-grained stages) (default:\n",
      "                        None)\n",
      "  --policy.dense_subtask_names [list]\n",
      "  --policy.dense_temporal_proportions [list]\n",
      "  --policy.pretrained_model_path [str]\n",
      "  --policy.image_key str\n",
      "                        Key for image used from the dataset (default:\n",
      "                        observation.images.top)\n",
      "  --policy.state_key str\n",
      "  --policy.normalization_mapping Dict\n",
      "\n",
      "Optional ['optimizer']:\n",
      "\n",
      "OptimizerConfig ['optimizer']:\n",
      "\n",
      "  --optimizer.type {adam,adamw,sgd,xvla-adamw,multi_adam}\n",
      "                        Which type of OptimizerConfig ['optimizer'] to use\n",
      "                        (default: None)\n",
      "\n",
      "AdamConfig ['optimizer']:\n",
      "\n",
      "  --optimizer.lr float  \n",
      "  --optimizer.weight_decay float\n",
      "  --optimizer.grad_clip_norm float\n",
      "  --optimizer.betas float float\n",
      "  --optimizer.eps float\n",
      "\n",
      "AdamWConfig ['optimizer']:\n",
      "\n",
      "  --optimizer.lr float  \n",
      "  --optimizer.weight_decay float\n",
      "  --optimizer.grad_clip_norm float\n",
      "  --optimizer.betas float float\n",
      "  --optimizer.eps float\n",
      "\n",
      "SGDConfig ['optimizer']:\n",
      "\n",
      "  --optimizer.lr float  \n",
      "  --optimizer.weight_decay float\n",
      "  --optimizer.grad_clip_norm float\n",
      "  --optimizer.momentum float\n",
      "  --optimizer.dampening float\n",
      "  --optimizer.nesterov bool\n",
      "\n",
      "XVLAAdamWConfig ['optimizer']:\n",
      "  Custom AdamW optimizer for XVLA with differential learning rates.\n",
      "  \n",
      "      The Vision-Language Model (VLM) is trained with 1/10 of the base learning rate\n",
      "      for stable optimization, while all other components use the full LR.\n",
      "  \n",
      "      This LR ratio is crucial for achieving strong and stable finetuning performance.\n",
      "  \n",
      "      Soft-prompts can optionally use a separate learning rate with warm-up support.\n",
      "      Set `soft_prompt_lr_scale` to a value < 1.0 (e.g., 0.1) to start soft-prompts\n",
      "      at a lower LR. Combine with a warmup scheduler for optimal results.\n",
      "  \n",
      "      Note:\n",
      "          Completely matching official reported performance may require an additional\n",
      "          warm-up LR schedule for soft-prompts, which can bring minor improvements.\n",
      "          When `soft_prompt_warmup_lr_scale` is set, soft-prompts start at\n",
      "          `lr * soft_prompt_warmup_lr_scale` and should be warmed up via the scheduler.\n",
      "  \n",
      "      Parameter Groups:\n",
      "          - Group 0 (vlm): VLM parameters at lr * 0.1, weight_decay * 0.1\n",
      "          - Group 1 (soft_prompts): Soft-prompt parameters at lr * soft_prompt_lr_scale\n",
      "          - Group 2 (other): All other parameters at full lr\n",
      "      \n",
      "\n",
      "  --optimizer.lr float  \n",
      "  --optimizer.weight_decay float\n",
      "  --optimizer.grad_clip_norm float\n",
      "  --optimizer.betas float float\n",
      "  --optimizer.eps float\n",
      "  --optimizer.soft_prompt_lr_scale float\n",
      "                        Scale factor for soft-prompt LR (1.0 = same as base\n",
      "                        LR) (default: 1.0)\n",
      "  --optimizer.soft_prompt_warmup_lr_scale [float]\n",
      "                        If set, start soft-prompts at this scale (e.g., 0.01)\n",
      "                        (default: None)\n",
      "\n",
      "MultiAdamConfig ['optimizer']:\n",
      "  Configuration for multiple Adam optimizers with different parameter groups.\n",
      "  \n",
      "      This creates a dictionary of Adam optimizers, each with its own hyperparameters.\n",
      "  \n",
      "      Args:\n",
      "          lr: Default learning rate (used if not specified for a group)\n",
      "          weight_decay: Default weight decay (used if not specified for a group)\n",
      "          optimizer_groups: Dictionary mapping parameter group names to their hyperparameters\n",
      "          grad_clip_norm: Gradient clipping norm\n",
      "      \n",
      "\n",
      "  --optimizer.lr float  \n",
      "  --optimizer.weight_decay float\n",
      "  --optimizer.grad_clip_norm float\n",
      "                        lr: float = 1e-3 weight_decay: float = 0.0\n",
      "                        grad_clip_norm: float = 10.0 optimizer_groups:\n",
      "                        dict[str, dict[str, Any]] =\n",
      "                        field(default_factory=dict) def build(self, params:\n",
      "                        OptimizerParams) -> dict[str, torch.optim.Optimizer]:\n",
      "                        (default: 10.0)\n",
      "  --optimizer.optimizer_groups Dict\n",
      "\n",
      "Optional ['scheduler']:\n",
      "\n",
      "LRSchedulerConfig ['scheduler']:\n",
      "\n",
      "  --scheduler.type {diffuser,vqbet,cosine_decay_with_warmup}\n",
      "                        Which type of LRSchedulerConfig ['scheduler'] to use\n",
      "                        (default: None)\n",
      "\n",
      "DiffuserSchedulerConfig ['scheduler']:\n",
      "\n",
      "  --scheduler.num_warmup_steps [int]\n",
      "  --scheduler.name str  \n",
      "\n",
      "VQBeTSchedulerConfig ['scheduler']:\n",
      "\n",
      "  --scheduler.num_warmup_steps int\n",
      "  --scheduler.num_vqvae_training_steps int\n",
      "  --scheduler.num_cycles float\n",
      "\n",
      "CosineDecayWithWarmupSchedulerConfig ['scheduler']:\n",
      "  Used by Physical Intelligence to train Pi0.\n",
      "  \n",
      "      Automatically scales warmup and decay steps if num_training_steps < num_decay_steps.\n",
      "      This ensures the learning rate schedule completes properly even with shorter training runs.\n",
      "      \n",
      "\n",
      "  --scheduler.num_warmup_steps int\n",
      "  --scheduler.num_decay_steps int\n",
      "  --scheduler.peak_lr float\n",
      "  --scheduler.decay_lr float\n",
      "\n",
      "EvalConfig ['eval']:\n",
      "\n",
      "  --eval.n_episodes int\n",
      "  --eval.batch_size int\n",
      "                        `batch_size` specifies the number of environments to\n",
      "                        use in a gym.vector.VectorEnv. (default: 50)\n",
      "  --eval.use_async_envs bool\n",
      "                        `use_async_envs` specifies whether to use asynchronous\n",
      "                        environments (multiprocessing). (default: False)\n",
      "\n",
      "WandBConfig ['wandb']:\n",
      "\n",
      "  --wandb.enable bool   \n",
      "  --wandb.disable_artifact bool\n",
      "                        Set to true to disable saving an artifact despite\n",
      "                        training.save_checkpoint=True (default: False)\n",
      "  --wandb.project str   \n",
      "  --wandb.entity [str]\n",
      "  --wandb.notes [str]\n",
      "  --wandb.run_id [str]\n",
      "  --wandb.mode [str]    Allowed values: 'online', 'offline' 'disabled'.\n",
      "                        Defaults to 'online' (default: None)\n",
      "\n",
      "Optional ['peft']:\n",
      "\n",
      "PeftConfig ['peft']:\n",
      "\n",
      "  --peft.target_modules [List|str]\n",
      "                        PEFT offers many fine-tuning methods, layer adapters\n",
      "                        being the most common and currently also the most\n",
      "                        effective methods so we'll focus on those in this\n",
      "                        high-level config interface. Either a string (module\n",
      "                        name suffix or 'all-linear'), a list of module name\n",
      "                        suffixes or a regular expression describing module\n",
      "                        names to target with the configured PEFT method. Some\n",
      "                        policies have a default value for this so that you\n",
      "                        don't *have* to choose which layers to adapt but it\n",
      "                        might still be worthwhile depending on your case.\n",
      "                        (default: None)\n",
      "  --peft.full_training_modules [List]\n",
      "                        Names/suffixes of modules to fully fine-tune and store\n",
      "                        alongside adapter weights. Useful for layers that are\n",
      "                        not part of a pre-trained model (e.g., action state\n",
      "                        projections). Depending on the policy this defaults to\n",
      "                        layers that are newly created in pre-trained policies.\n",
      "                        If you're fine-tuning an already trained policy you\n",
      "                        might want to set this to `[]`. Corresponds to PEFT's\n",
      "                        `modules_to_save`. (default: None)\n",
      "  --peft.method_type str\n",
      "                        The PEFT (adapter) method to apply to the policy.\n",
      "                        Needs to be a valid PEFT type. (default: LORA)\n",
      "  --peft.init_type [str]\n",
      "                        Adapter initialization method. Look at the specific\n",
      "                        PEFT adapter documentation for defaults. (default:\n",
      "                        None)\n",
      "  --peft.r int          We expect that all PEFT adapters are in some way doing\n",
      "                        rank-decomposition therefore this parameter specifies\n",
      "                        the rank used for the adapter. In general a higher\n",
      "                        rank means more trainable parameters and closer to\n",
      "                        full fine-tuning. (default: 16)\n"
     ]
    }
   ],
   "source": [
    "!lerobot-train -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c7c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdillon-desilvaa\u001b[0m (\u001b[33mdillon-desilvaa-1\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88334d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(token=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "232120fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/parsers/decoding.py\", line 357, in _try_functions\n",
      "    return func(val, path)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/parsers/decoding.py\", line 185, in decode_choice_class\n",
      "    raise ParsingError(f\"Expected a dict with a '{CHOICE_TYPE_KEY}' key for {cls}, got {raw_value}\")\n",
      "draccus.utils.ParsingError: Expected a dict with a 'type' key for <class 'lerobot.configs.policies.PreTrainedConfig'>, got {'repo_id': 'lerobot/smolvla_base', 'device': 'cpu'}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lerobot-train\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lerobot/scripts/train.py\", line 296, in main\n",
      "    train()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lerobot/configs/parser.py\", line 224, in wrapper_inner\n",
      "    cfg = draccus.parse(config_class=argtype, config_path=config_path, args=cli_args)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/argparsing.py\", line 211, in parse\n",
      "    return parser.parse_args(args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/argparsing.py\", line 102, in parse_args\n",
      "    args, _ = self.parse_known_args(args, namespace, is_parse_args=True)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/argparsing.py\", line 136, in parse_known_args\n",
      "    parsed_t = self._postprocessing(parsed_args)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/argparsing.py\", line 180, in _postprocessing\n",
      "    cfg = decoding.decode(self.config_class, deflat_d)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/parsers/registry_utils.py\", line 78, in wrapper\n",
      "    return base_func(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/parsers/decoding.py\", line 48, in decode\n",
      "    return get_decoding_fn(cls)(raw_value, ())  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/parsers/decoding.py\", line 135, in decode_dataclass\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/parsers/decoding.py\", line 133, in decode_dataclass\n",
      "    field_value = get_decoding_fn(field_type)(raw_value, (*path, name))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/draccus/parsers/decoding.py\", line 384, in _try_functions\n",
      "    raise DecodingError(path, message) from exception_to_raise_from\n",
      "draccus.utils.DecodingError: `policy`: Could not decode the value into any of the given types:\n",
      "    PreTrainedConfig: Expected a dict with a 'type' key for <class 'lerobot.configs.policies.PreTrainedConfig'>, got {'repo_id': 'lerobot/smolvla_base', 'device': 'cpu'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lerobot-train \\\n",
    "    --policy.repo_id=lerobot/smolvla_base \\\n",
    "    --dataset.repo_id=dillondesilva/so101-table-cleanup \\\n",
    "    --batch_size=1 \\\n",
    "    --steps=10 \\\n",
    "    --output_dir=outputs/train/grootn1.5-pick-and-place-so101 \\\n",
    "    --job_name=test-place-so101_training \\\n",
    "    --policy.device=cpu \\\n",
    "    --wandb.enable=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272540f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
